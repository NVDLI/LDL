{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example is very similar to c17e4_nas_random_hill, but it uses an evolutionary search algorithm instead of random search and hill climbing. More context for this code example can be found in the section \"Programming Example: Searching for an architecture for CIFAR-10 classification\" in Chapter 17 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n",
    "\n",
    "The initial part of the programming example is identical to c17e4_nas_random_hill with the addition of a constant POPULATION_SIZE which is used by the evolutionary algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "import numpy as np\n",
    "import logging\n",
    "import copy\n",
    "import random\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "MAX_MODEL_SIZE = 500000\n",
    "CANDIDATE_EVALUATIONS = 500\n",
    "EVAL_EPOCHS = 3\n",
    "FINAL_EPOCHS = 20\n",
    "POPULATION_SIZE = 50\n",
    "\n",
    "layer_types = ['DENSE', 'CONV2D', 'MAXPOOL2D']\n",
    "param_values = dict([('size', [16, 64, 256, 1024, 4096]),\n",
    "                ('activation', ['relu', 'tanh', 'elu']),\n",
    "                ('kernel_size', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
    "                ('stride', [(1, 1), (2, 2), (3, 3), (4, 4)]),\n",
    "                ('dropout', [0.0, 0.4, 0.7, 0.9])])\n",
    "\n",
    "layer_params = dict([('DENSE', ['size', 'activation', 'dropout']),\n",
    "                     ('CONV2D', ['size', 'activation',\n",
    "                                 'kernel_size', 'stride',\n",
    "                                 'dropout']),\n",
    "                     ('MAXPOOL2D', ['kernel_size', 'stride',\n",
    "                                    'dropout'])])\n",
    "\n",
    "# Load dataset.\n",
    "cifar_dataset = keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images,\n",
    "                    test_labels) = cifar_dataset.load_data()\n",
    "\n",
    "# Standardize dataset.\n",
    "mean = np.mean(train_images)\n",
    "stddev = np.std(train_images)\n",
    "train_images = (train_images - mean) / stddev\n",
    "test_images = (test_images - mean) / stddev\n",
    "\n",
    "# Change labels to one-hot.\n",
    "train_labels = to_categorical(train_labels,\n",
    "                              num_classes=10)\n",
    "test_labels = to_categorical(test_labels,\n",
    "                             num_classes=10)\n",
    "\n",
    "# Methods to create a model definition.\n",
    "def generate_random_layer(layer_type):\n",
    "    layer = {}\n",
    "    layer['layer_type'] = layer_type\n",
    "    params = layer_params[layer_type]\n",
    "    for param in params:\n",
    "        values = param_values[param]\n",
    "        layer[param] = values[np.random.randint(0, len(values))]\n",
    "    return layer\n",
    "\n",
    "def generate_model_definition():\n",
    "    layer_count = np.random.randint(2, 9)\n",
    "    non_dense_count = np.random.randint(1, layer_count)\n",
    "    layers = []\n",
    "    for i in range(layer_count):\n",
    "        if i < non_dense_count:\n",
    "            layer_type = layer_types[np.random.randint(1, 3)]\n",
    "            layer = generate_random_layer(layer_type)\n",
    "        else:\n",
    "            layer = generate_random_layer('DENSE')\n",
    "        layers.append(layer)\n",
    "    return layers\n",
    "\n",
    "def compute_weight_count(layers):\n",
    "    last_shape = (32, 32, 3)\n",
    "    total_weights = 0\n",
    "    for layer in layers:\n",
    "        layer_type = layer['layer_type']\n",
    "        if layer_type == 'DENSE':\n",
    "            size = layer['size']\n",
    "            weights = size * (np.prod(last_shape) + 1)\n",
    "            last_shape = (layer['size'])\n",
    "        else:\n",
    "            stride = layer['stride']\n",
    "            if layer_type == 'CONV2D':\n",
    "                size = layer['size']\n",
    "                kernel_size = layer['kernel_size']\n",
    "                weights = size * ((np.prod(kernel_size) *\n",
    "                                   last_shape[2]) + 1)\n",
    "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
    "                              np.ceil(last_shape[1]/stride[1]),\n",
    "                              size)\n",
    "            elif layer_type == 'MAXPOOL2D':\n",
    "                weights = 0\n",
    "                last_shape = (np.ceil(last_shape[0]/stride[0]),\n",
    "                              np.ceil(last_shape[1]/stride[1]),\n",
    "                              last_shape[2])\n",
    "        total_weights += weights\n",
    "    total_weights += ((np.prod(last_shape) + 1) * 10)\n",
    "    return total_weights\n",
    "\n",
    "# Methods to create and evaluate model based on model definition.\n",
    "def add_layer(model, params, prior_type):\n",
    "    layer_type = params['layer_type']\n",
    "    if layer_type == 'DENSE':\n",
    "        if prior_type != 'DENSE':\n",
    "            model.add(Flatten())\n",
    "        size = params['size']\n",
    "        act = params['activation']\n",
    "        model.add(Dense(size, activation=act))\n",
    "    elif layer_type == 'CONV2D':\n",
    "        size = params['size']\n",
    "        act = params['activation']\n",
    "        kernel_size = params['kernel_size']\n",
    "        stride = params['stride']\n",
    "        model.add(Conv2D(size, kernel_size, activation=act,\n",
    "                         strides=stride, padding='same'))\n",
    "    elif layer_type == 'MAXPOOL2D':\n",
    "        kernel_size = params['kernel_size']\n",
    "        stride = params['stride']\n",
    "        model.add(MaxPooling2D(pool_size=kernel_size,\n",
    "                               strides=stride, padding='same'))\n",
    "    dropout = params['dropout']\n",
    "    if(dropout > 0.0):\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "def create_model(layers):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x, input_shape=(32, 32, 3)))\n",
    "    prev_layer = 'LAMBDA' # Dummy layer to set input_shape\n",
    "    for layer in layers:\n",
    "        add_layer(model, layer, prev_layer)\n",
    "        prev_layer = layer['layer_type']\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_and_evaluate_model(model_definition):\n",
    "    weight_count = compute_weight_count(model_definition)\n",
    "    if weight_count > MAX_MODEL_SIZE:\n",
    "        return 0.0\n",
    "    model = create_model(model_definition)\n",
    "    history = model.fit(train_images, train_labels,\n",
    "                        validation_data=(test_images, test_labels),\n",
    "                        epochs=EVAL_EPOCHS, batch_size=64,\n",
    "                        verbose=2, shuffle=False)\n",
    "    acc = history.history['val_accuracy'][-1]\n",
    "    print('Size: ', weight_count)\n",
    "    print('Accuracy: %5.2f' %acc)\n",
    "    return acc\n",
    "\n",
    "# Helper method for hill climbing and evolutionary algorithm.\n",
    "def tweak_model(model_definition):\n",
    "    layer_num = np.random.randint(0, len(model_definition))\n",
    "    last_layer = len(model_definition) - 1\n",
    "    for first_dense, layer in enumerate(model_definition):\n",
    "        if layer['layer_type'] == 'DENSE':\n",
    "            break\n",
    "    if np.random.randint(0, 2) == 1:\n",
    "        delta = 1\n",
    "    else:\n",
    "        delta = -1\n",
    "    if np.random.randint(0, 2) == 1:\n",
    "        # Add/remove layer.\n",
    "        if len(model_definition) < 3:\n",
    "            delta = 1 # Layer removal not allowed\n",
    "        if delta == -1:\n",
    "            # Remove layer.\n",
    "            if layer_num == 0 and first_dense == 1:\n",
    "                layer_num += 1 # Require >= 1 non-dense layer\n",
    "            if layer_num == first_dense and layer_num == last_layer:\n",
    "                layer_num -= 1 # Require >= 1 dense layer\n",
    "            del model_definition[layer_num]\n",
    "        else:\n",
    "            # Add layer.\n",
    "            if layer_num < first_dense:\n",
    "                layer_type = layer_types[np.random.randint(1, 3)]\n",
    "            else:\n",
    "                layer_type = 'DENSE'\n",
    "            layer = generate_random_layer(layer_type)\n",
    "            model_definition.insert(layer_num, layer)\n",
    "    else:\n",
    "        # Tweak parameter.\n",
    "        layer = model_definition[layer_num]\n",
    "        layer_type = layer['layer_type']\n",
    "        params = layer_params[layer_type]\n",
    "        param = params[np.random.randint(0, len(params))]\n",
    "        current_val = layer[param]\n",
    "        values = param_values[param]\n",
    "        index = values.index(current_val)\n",
    "        max_index = len(values)\n",
    "        new_val = values[(index + delta) % max_index]\n",
    "        layer[param] = new_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of the evolutionary algorithm is the crossover operation, which combines two existing solutions (parents) into a new solution (child) that inherits properties of both of its parents. It is implemented in the code snippet below (see book for more details).\n",
    "\n",
    "The evolutionary algorithm starts by generating and evaluating a population of random models. It then randomly generates new models by tweaking and combining models in the existing population. There are three ways that a new model can be created:\n",
    "- Tweak an existing model.\n",
    "- Combine two parent models into a child model.\n",
    "- Combine two parent models into a child model and apply a tweak to the resulting model.\n",
    "\n",
    "Once new models have been generated, the algorithm probabilistically selects high-performing models to keep for the next iteration. In this selection process, both the parents and the children participate, which is also known as elitism within the field of evolutionary computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for evolutionary algorithm.\n",
    "def cross_over(parents):\n",
    "    # Pick bottom half of one and top half of the other.\n",
    "    # If model is small, randomly stack top or bottom from both.\n",
    "    bottoms = [[], []]\n",
    "    tops = [[], []]\n",
    "    for i, model in enumerate(parents):\n",
    "        for layer in model:\n",
    "            if layer['layer_type'] != 'DENSE':\n",
    "                bottoms[i].append(copy.deepcopy(layer))\n",
    "            else:\n",
    "                tops[i].append(copy.deepcopy(layer))\n",
    "\n",
    "    i = np.random.randint(0, 2)\n",
    "    if (i == 1 and compute_weight_count(parents[0]) +\n",
    "        compute_weight_count(parents[1]) < MAX_MODEL_SIZE):\n",
    "        i = np.random.randint(0, 2)\n",
    "        new_model = bottoms[i] + bottoms[(i+1)%2]\n",
    "        i = np.random.randint(0, 2)\n",
    "        new_model = new_model + tops[i] + tops[(i+1)%2]\n",
    "    else:\n",
    "        i = np.random.randint(0, 2)\n",
    "        new_model = bottoms[i] + tops[(i+1)%2]\n",
    "    return new_model\n",
    "\n",
    "# Evolutionary algorithm.\n",
    "np.random.seed(7)\n",
    "\n",
    "# Generate initial population of models.\n",
    "population = []\n",
    "for i in range(POPULATION_SIZE):\n",
    "    valid_model = False\n",
    "    while(valid_model == False):\n",
    "        model_definition = generate_model_definition()\n",
    "        acc = create_and_evaluate_model(model_definition)\n",
    "        if acc > 0.0:\n",
    "            valid_model = True\n",
    "    population.append((acc, model_definition))\n",
    "\n",
    "# Evolve population.\n",
    "generations = int(CANDIDATE_EVALUATIONS / POPULATION_SIZE) - 1\n",
    "for i in range(generations):\n",
    "    # Generate new individuals.\n",
    "    print('Generation number: ', i)\n",
    "    for j in range(POPULATION_SIZE):\n",
    "        valid_model = False\n",
    "        while(valid_model == False):\n",
    "            rand = np.random.rand()\n",
    "            parents = random.sample(\n",
    "                population[:POPULATION_SIZE], 2)\n",
    "            parents = [parents[0][1], parents[1][1]]\n",
    "            if rand < 0.5:\n",
    "                child = copy.deepcopy(parents[0])\n",
    "                tweak_model(child)\n",
    "            elif rand < 0.75:\n",
    "                child = cross_over(parents)\n",
    "            else:\n",
    "                child = cross_over(parents)\n",
    "                tweak_model(child)\n",
    "            acc = create_and_evaluate_model(child)\n",
    "            if acc > 0.0:\n",
    "                valid_model = True\n",
    "        population.append((acc, child))\n",
    "    # Randomly select fit individuals.\n",
    "    population.sort(key=lambda x:x[0])\n",
    "    print('Evolution, best accuracy: %5.2f' %population[-1][0])\n",
    "    top = np.int64(np.ceil(0.2*len(population)))\n",
    "    bottom = np.int64(np.ceil(0.3*len(population)))\n",
    "    top_individuals = population[-top:]\n",
    "    remaining = np.int64(len(population)/2) - len(top_individuals)\n",
    "    population = random.sample(population[bottom:-top],\n",
    "                               remaining) + top_individuals\n",
    "\n",
    "best_model = population[-1][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for c17e4_nas_random_hill, we conclude with evaluating the best model for a larger number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final model for larger number of epochs.\n",
    "model = create_model(best_model)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    train_images, train_labels, validation_data =\n",
    "    (test_images, test_labels), epochs=FINAL_EPOCHS, batch_size=64,\n",
    "    verbose=2, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
