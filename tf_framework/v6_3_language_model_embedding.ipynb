{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example is similar to v5_8_text_autocompletion, but it works on words (encoded with an embedding layer) instead of characters and it does not do beam search. More context for this code example can be found in video 6.3 \"Programming Example: Language Model and Word Embeddings with TensorFlow\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization code below contains a couple of additional imports compared to v5_8_text_autocompletion and defines two new constants MAX_WORDS and EMBEDDING_WIDTH that define the max size of our vocabulary and the dimensionality of the word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = '../data/frankenstein.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "PREDICT_LENGTH = 3\n",
    "MAX_WORDS = 7500\n",
    "EMBEDDING_WIDTH = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet first reads the input file and splits the text into a list of individual words. The latter is done by using the imported function text_to_word_sequence(), which also removes punctuation and converts the text to lowercase. We then create input fragments and associated target words just as in the character-based example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read file.\n",
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8-sig')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Make lower case and split into individual words.\n",
    "text = text_to_word_sequence(text)\n",
    "\n",
    "# Create training examples.\n",
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "    targets.append(text[i + WINDOW_LENGTH])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the training examples into the correct format. Each input word needs to be encoded to a corresponding word index (an integer). This index will then be converted into an embedding by the Embedding layer. The target (output) word should still be one-hot encoded. To simplify how to interpret the output, we want the one-hot encoding to be done in such a way that bit N is hot when the network outputs the word corresponding to index N in the input encoding.\n",
    "\n",
    "We make use of the Keras Tokenizer class. When we construct our tokenizer, we provide an argument num_words = MAX_WORDS that caps the size of the vocabulary. The tokenizer object reserves index 0 to use as a special padding value and index 1 for unknown words. The remaining 7,498 indices (MAX_WORDS was set to 7,500) are used to represent words in the vocabulary.\n",
    "\n",
    "The padding value (index 0) can be used to make all training examples within the same batch have the same length. The Embedding layer can be instructed to ignore this value, so the network does not train on the padding values.\n",
    "\n",
    "Index 1 is reserved for UNKnown (UNK) words because we have declared UNK as an out-of-vocabulary (oov) token. When using the tokenizer to convert text to tokens, any word that is not in the vocabulary will be replaced by the word UNK. Similarly, if we try to convert an index that is not assigned to a word, the tokenizer will return UNK. If we do not set the oov_token parameter, it will simply ignore such words/indices.\n",
    "\n",
    "After instantiating our tokenizer, we call fit_on_texts() with our entire text corpus, which will result in the tokenizer assigning indices to words. We can then use the function texts_to_sequences to convert a text string into a list of indices, where unknown words will be assigned the index 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to indices.\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(text)\n",
    "fragments_indexed = tokenizer.texts_to_sequences(fragments)\n",
    "targets_indexed = tokenizer.texts_to_sequences(targets)\n",
    "\n",
    "# Convert to appropriate input and output formats.\n",
    "X = np.array(fragments_indexed, dtype=np.int64)\n",
    "y = np.zeros((len(targets_indexed), MAX_WORDS))\n",
    "for i, target_index in enumerate(targets_indexed):\n",
    "    y[i, target_index] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet creates a model with an Embedding layer followed by two long short-term memory (LSTM) layers, followed by one fully connected layer with ReLU activation, and finally a fully connected layer with softmax as output. When we declare the Embedding layer, we provide it with its input dimensions (vocabulary size) and output dimensions (embedding width) and tell it to mask inputs using index 0. This masking is not necessary for our programming example given that we created the training input such that all input examples have the same length, but we do it for good practice. We state input_length=None so that we can feed training examples of any length to the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train model.\n",
    "training_model = Sequential()\n",
    "training_model.add(Input(shape=(None,), batch_size=BATCH_SIZE))\n",
    "training_model.add(Embedding(\n",
    "    output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS,\n",
    "    mask_zero=True))\n",
    "training_model.add(LSTM(128, return_sequences=True,\n",
    "                        dropout=0.2, recurrent_dropout=0.2))\n",
    "training_model.add(LSTM(128, dropout=0.2,\n",
    "                        recurrent_dropout=0.2))\n",
    "training_model.add(Dense(128, activation='relu'))\n",
    "training_model.add(Dense(MAX_WORDS, activation='softmax'))\n",
    "training_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='adam')\n",
    "training_model.summary()\n",
    "history = training_model.fit(X, y, validation_split=0.05,\n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             epochs=EPOCHS, verbose=2, \n",
    "                             shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we are ready to use it to do predictions. We do this a little bit differently than in the previous example. Instead of feeding a string of symbols as input to the model, we feed it only a single symbol at a time. This is an alternative implementation compared to the implementation in v5_8_text_autocompletion, where we repeatedly fed the model a growing sequence of characters.\n",
    "\n",
    "The scheme used in this chapter has a subtle implication, which has to do with dependencies between multiple consecutive calls to model.predict(). We want the LSTM layers to retain their c and h states from one call to another so that the outputs of subsequent calls to predict() will depend on the prior calls to predict(). This can be done by giving the parameter stateful=True to the LSTM layers. A side effect of this is that we manually need to call reset_states() on the model before our first prediction.\n",
    "\n",
    "The code snippet below creates a model that is identical to the training model except that we declare the LSTM layers with stateful=True as well as specify a fixed batch size (required when declaring the LSTM layer as stateful) of size 1 using the batch_input_shape argument. We will transfer the weights from the trained model to this new model and use it for inference. This is done in the two last lines in the code snippet. There, we first read out the weights from the trained model and then initialize it into our inference model. For this to work, the models must have identical topology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stateful model used for prediction.\n",
    "inference_model = Sequential()\n",
    "inference_model.add(Input(shape=(None,), batch_size=1))\n",
    "inference_model.add(Embedding(\n",
    "    output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS,\n",
    "    mask_zero=False))\n",
    "inference_model.add(LSTM(128, return_sequences=True,\n",
    "                         dropout=0.2, recurrent_dropout=0.2,\n",
    "                         stateful=True))\n",
    "inference_model.add(LSTM(128, dropout=0.2,\n",
    "                         recurrent_dropout=0.2, stateful=True))\n",
    "inference_model.add(Dense(128, activation='relu'))\n",
    "inference_model.add(Dense(MAX_WORDS, activation='softmax'))\n",
    "weights = training_model.get_weights()\n",
    "inference_model.set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet implements logic of presenting a word to the model and retrieving the word with the highest probability from the output. This word is then fed back as input to the model in the next timestep. The resulting autocompleted text sequence is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide beginning of sentence and\n",
    "# predict next words in a greedy manner\n",
    "first_words = ['i', 'saw']\n",
    "first_words_indexed = tokenizer.texts_to_sequences(\n",
    "    first_words)\n",
    "inference_model.layers[1].reset_states()\n",
    "inference_model.layers[2].reset_states()\n",
    "predicted_string = ''\n",
    "# Feed initial words to the model.\n",
    "for i, word_index in enumerate(first_words_indexed):\n",
    "    x = np.zeros((1, 1), dtype=np.int64)\n",
    "    x[0][0] = word_index[0]\n",
    "    predicted_string += first_words[i]\n",
    "    predicted_string += ' '\n",
    "    y_predict = inference_model.predict(x, verbose=0)[0]\n",
    "# Predict PREDICT_LENGTH words.\n",
    "for i in range(PREDICT_LENGTH):\n",
    "    new_word_index = np.argmax(y_predict)\n",
    "    word = tokenizer.sequences_to_texts(\n",
    "        [[new_word_index]])\n",
    "    x[0][0] = new_word_index\n",
    "    predicted_string += word[0]\n",
    "    predicted_string += ' '\n",
    "    y_predict = inference_model.predict(x, verbose=0)[0]\n",
    "print(predicted_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the preceding code had to do with building and using a language model. The next code snippet adds some functionality to explore the learned embeddings. We first read out the word embeddings from the Embedding layer by calling get_weights() on layer 0, which represents the Embedding layer. We then declare a list of a number of arbitrary lookup words. This is followed by a loop that does one iteration per lookup word. The loop uses the Tokenizer to convert the lookup word to a word index, which is then used to retrieve the corresponding word embedding. The Tokenizer functions are generally assumed to work on lists. Therefore, although we work with a single word at a time, we need to provide it as a list of size 1, and then we need to retrieve element zero ([0]) from the output.\n",
    "\n",
    "Once we have retrieved the corresponding word embedding, we loop through all the other embeddings and calculate the Euclidean distance to the embedding for the lookup word using the NumPy function norm(). We add the distance and the corresponding word to the dictionary word_indices. Once we have calculated the distance to each word, we simply sort the distances and retrieve the five word indices that correspond to the word embeddings that are closest in vector space. We use the Tokenizer to convert these indices back to words and print them and their corresponding distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore embedding similarities.\n",
    "embeddings = training_model.layers[0].get_weights()[0]\n",
    "lookup_words = ['the', 'of']\n",
    "for lookup_word in lookup_words:\n",
    "    lookup_word_indexed = tokenizer.texts_to_sequences(\n",
    "        [lookup_word])\n",
    "    print('words close to:', lookup_word)\n",
    "    lookup_embedding = embeddings[lookup_word_indexed[0]]\n",
    "    word_indices = {}\n",
    "    # Calculate distances.\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        distance = np.linalg.norm(\n",
    "            embedding - lookup_embedding)\n",
    "        word_indices[distance] = i\n",
    "    # Print sorted by distance.\n",
    "    for distance in sorted(word_indices.keys())[:5]:\n",
    "        word_index = word_indices[distance]\n",
    "        word = tokenizer.sequences_to_texts([[word_index]])[0]\n",
    "        print(word + ': ', distance)\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
