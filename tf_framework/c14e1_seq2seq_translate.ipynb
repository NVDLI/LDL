{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to build a neural machine translation network. It is a sequence-to-sequence network based on an encoder-decoder architecture. More context for this code example can be found in the section \"Programming Example: Neural Machine Translation\" in Chapter 14 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n",
    "\n",
    "The data used to train the model is expected to be in the file ../data/fra.txt.\n",
    "We begin by importing modules that we need for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence \\\n",
    "    import pad_sequences\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some constants. We specify a vocabulary size of 10,000 symbols, out of which four indices are reserved for padding, out-of-vocabulary words (denoted as UNK), START tokens, and STOP tokens. Our training corpus is large, so we set the parameter READ_LINES to the number of lines in the input file we want to use in our example (60,000). Our layers consist of 256 units (LAYER_SIZE), and the embedding layers output 128 dimensions (EMBEDDING_WIDTH). We use 20% (TEST_PERCENT) of the dataset as test set and further select 20 sentences (SAMPLE_SIZE) to inspect in detail during training. We limit the length of the source and destination sentences to, at most, 60 words (MAX_LENGTH). Finally, we provide the path to the data file, where each line is expected to contain two versions of the same sentence (one in each language) separated by a tab character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_LINES = 60000\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128\n",
    "TEST_PERCENT = 0.2\n",
    "SAMPLE_SIZE = 20\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "SRC_DEST_FILE_NAME = '../data/fra.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the function used to read the input data file and do some initial processing. Each line is split into two strings, where the first contains the sentence in the destination language and the second contains the sentence in the source language. We use the function text_to_word_sequence() to clean the data somewhat (make everything lowercase and remove punctuation) and split each sentence into a list of individual words. If the list (sentence) is longer than the maximum allowed length, then it is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read file.\n",
    "def read_file_combined(file_name, max_len):\n",
    "    file = open(file_name, 'r', encoding='utf-8')\n",
    "    src_word_sequences = []\n",
    "    dest_word_sequences = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i == READ_LINES:\n",
    "            break\n",
    "        pair = line.split('\\t')\n",
    "        word_sequence = text_to_word_sequence(pair[1])\n",
    "        src_word_sequence = word_sequence[0:max_len]\n",
    "        src_word_sequences.append(src_word_sequence)\n",
    "        word_sequence = text_to_word_sequence(pair[0])\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    file.close()\n",
    "    return src_word_sequences, dest_word_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows functions used to turn sequences of words into\n",
    "sequences of tokens, and vice versa. We call tokenize() a single time for each\n",
    "language, so the argument sequences is a list of lists where each of the inner\n",
    "lists represents a sentence. The Tokenizer class assigns indices to the most\n",
    "common words and returns either these indices or the reserved OOV_INDEX\n",
    "for less common words that did not make it into the vocabulary. We tell the\n",
    "Tokenizer to use a vocabulary of 9998 (MAX_WORDS-2)â€”that is, use only\n",
    "indices 0 to 9997, so that we can use indices 9998 and 9999 as our START and\n",
    "STOP tokens (the Tokenizer does not support the notion of START and STOP\n",
    "tokens but does reserve index 0 to use as a padding token and index 1 for outof-\n",
    "vocabulary words). Our tokenize() function returns both the tokenized\n",
    "sequence and the Tokenizer object itself. This object will be needed anytime we\n",
    "want to convert tokens back into words.\n",
    "\n",
    "The function tokens_to_words() requires a Tokenizer and a list of indices. We simply check for the reserved indices: If we find a match, we replace them with hardcoded strings, and if we find no match, we let the Tokenizer convert the index to the corresponding word string. The Tokenizer expects a list of lists of indices and returns a list of strings, which is why we need to call it with [[index]] and then select the 0th element to arrive at a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to tokenize and un-tokenize sequences.\n",
    "def tokenize(sequences):\n",
    "    # \"MAX_WORDS-2\" used to reserve two indices\n",
    "    # for START and STOP.\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
    "                          oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts(\n",
    "                [[index]])[0])\n",
    "    print(word_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these helper functions, it is trivial to read the input data\n",
    "file and convert into tokenized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and tokenize.\n",
    "src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME,\n",
    "                                       MAX_LENGTH)\n",
    "src_tokenizer, src_token_seq = tokenize(src_seq)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to arrange the data into tensors that can be used for training and testing. See the book for more details about the desired format. The code snippet below shows a compact way of creating the three arrays that we need. The first two lines create two new lists, each containing the destination sequences but the first (dest_target_token_seq) also augmented with STOP_INDEX after each sequence and the second (dest_input_token_seq) augmented with both START_INDEX and STOP_INDEX. It is easy to miss that dest_input_token_seq has a STOP_INDEX, but that falls out naturally because it is created from the dest_target_token_seq for which a STOP_INDEX was just added to each sentence.\n",
    "\n",
    "Next, we call pad_sequences() on both the original src_input_data list (of lists) and on these two new destination lists. The pad_sequences() function pads the sequences with the PAD value and then returns a NumPy array. The default behavior of pad_sequences is to do prepadding, but we explicitly ask for postpadding for both source and destination (in the original version we used prepadding for the source sequence but that caused issues in a more recent version of cuDNN).\n",
    "\n",
    "You might wonder why there is no call to to_categorical() in the statement that creates the target (output) data. We are used to wanting to have the ground truth one-hot encoded for textual data. Not doing so is an optimization to avoid wasting too much memory. With a vocabulary of 10,000 words, and 60,000 training examples, where each training example is a sentence, the memory footprint of the one-hot encoded data starts becoming a problem. Therefore, instead of one-hot encoding all data up front, there is a way to let Keras deal with that in the loss function itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in\n",
    "                        dest_target_token_seq]\n",
    "src_input_data = pad_sequences(src_token_seq, padding='post')\n",
    "dest_input_data = pad_sequences(dest_input_token_seq,\n",
    "                                padding='post')\n",
    "dest_target_data = pad_sequences(\n",
    "    dest_target_token_seq, padding='post', maxlen\n",
    "    = len(dest_input_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet demonstrates how we can manually split our dataset into a training dataset and a test dataset. In previous examples, we either relied on datasets that are already split this way or we used functionality inside of Keras when calling the fit() function. However, in this case, we want some more control ourselves because we will want to inspect a few select members of the test set in detail. We split the dataset by first creating a list test_indices, which contains a 20% (TEST_PERCENT) subset of all the numbers from 0 to Nâˆ’1, where N is the size of our original dataset. We then create a list train_indices, which contains the remaining 80%. We can now use these lists to select a number of rows in the matrices representing the dataset and create two new collections of matrices, one to be used as training set and one to be used as test set. Finally, we create a third collection of matrices, which only contains 20 (SAMPLE_SIZE) random examples from the test dataset. We will use them to inspect the resulting translations in detail, but since that is a manual process, we limit ourselves to a small number of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set.\n",
    "rows = len(src_input_data[:,0])\n",
    "all_indices = list(range(rows))\n",
    "test_rows = int(rows * TEST_PERCENT)\n",
    "test_indices = random.sample(all_indices, test_rows)\n",
    "train_indices = [x for x in all_indices if x not in test_indices]\n",
    "\n",
    "train_src_input_data = src_input_data[train_indices]\n",
    "train_dest_input_data = dest_input_data[train_indices]\n",
    "train_dest_target_data = dest_target_data[train_indices]\n",
    "\n",
    "test_src_input_data = src_input_data[test_indices]\n",
    "test_dest_input_data = dest_input_data[test_indices]\n",
    "test_dest_target_data = dest_target_data[test_indices]\n",
    "\n",
    "# Create a sample of the test set that we will inspect in detail.\n",
    "test_indices = list(range(test_rows))\n",
    "sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n",
    "sample_input_data = test_src_input_data[sample_indices]\n",
    "sample_target_data = test_dest_target_data[sample_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build our model. It consists of an encoder part and a decoder part (see detailed figures in the book). The encoder consists of an embedding layer and two LSTM layers. The decoder consists of an embedding layer, two LSTM layers, and a fully connected softmax layer. We define these as two separate models, which we later tie together. Four noteworthy outputs are the state outputs from the two encoder LSTM layers. These are used as inputs into the decoder LSTM layers to communicate the accumulated state from the encoder to the decoder. To be able to express this complex model we need to use the Keras Functional API.\n",
    "\n",
    "The code snippet below contains the implementation of the encoder model. There are a few things worth pointing out. Because we are interested in accessing the internal state of the LSTM layers, we need to provide the argument return_state=True. This argument instructs the LSTM object to return not only a variable representing the layerâ€™s output but also variables representing the c and h states. Further, for a recurrent layer that feeds another recurrent layer, we need to provide the argument return_sequences=True so that the subsequent layer sees the outputs of each timestep. This is also true for the final recurrent layer if we want the network to produce an output during each timestep. For our encoder, we are only interested in the final state, so we do not set return_sequences to True for enc_layer2.\n",
    "\n",
    "Once all layers are connected, we create the actual model by calling the Model() constructor and providing arguments to specify what inputs and outputs will be external to the model. The model takes the source sentence as input and produces the internal states of the two LSTM layers as outputs. Each LSTM layer has both an h state and c state, so in total, the model will output four state variables as output. Each state variable is in itself a tensor consisting of multiple values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder model.\n",
    "# Input is input sequence in source language.\n",
    "enc_embedding_input = Input(shape=(None, ))\n",
    "\n",
    "# Create the encoder layers.\n",
    "enc_embedding_layer = Embedding(\n",
    "    output_dim=EMBEDDING_WIDTH, input_dim\n",
    "    = MAX_WORDS, mask_zero=True)\n",
    "enc_layer1 = LSTM(LAYER_SIZE, return_state=True,\n",
    "                  return_sequences=True)\n",
    "enc_layer2 = LSTM(LAYER_SIZE, return_state=True)\n",
    "\n",
    "# Connect the encoder layers.\n",
    "# We don't use the last layer output, only the state.\n",
    "enc_embedding_layer_outputs = \\\n",
    "    enc_embedding_layer(enc_embedding_input)\n",
    "enc_layer1_outputs, enc_layer1_state_h, enc_layer1_state_c = \\\n",
    "    enc_layer1(enc_embedding_layer_outputs)\n",
    "_, enc_layer2_state_h, enc_layer2_state_c = \\\n",
    "    enc_layer2(enc_layer1_outputs)\n",
    "\n",
    "# Build the model.\n",
    "enc_model = Model(enc_embedding_input,\n",
    "                  [enc_layer1_state_h, enc_layer1_state_c,\n",
    "                   enc_layer2_state_h, enc_layer2_state_c])\n",
    "enc_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the implementation of the decoder model. In addition to the sentence in the destination language, it takes the output state from the encoder model as inputs. We initialize the decoder LSTM layers (using the argument initial_state) with this state at the first timestep.\n",
    "\n",
    "For the decoder, we do want the top LSTM layer to produce an output for each timestep (the decoder should create a full sentence and not just a final state), so we set return_sequences=True for both LSTM layers.\n",
    "\n",
    "We create the model by calling the Model() constructor. The inputs consist of the destination sentence (time shifted by one timestep) and initial state for the LSTM layers. As we soon will see, when using the model for inference, we need to explicitly manage the internal state for the decoder. Therefore, we declare the states as outputs of the model in addition to the softmax output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decoder model.\n",
    "# Input to the network is input sequence in destination\n",
    "# language and intermediate state.\n",
    "dec_layer1_state_input_h = Input(shape=(LAYER_SIZE,))\n",
    "dec_layer1_state_input_c = Input(shape=(LAYER_SIZE,))\n",
    "dec_layer2_state_input_h = Input(shape=(LAYER_SIZE,))\n",
    "dec_layer2_state_input_c = Input(shape=(LAYER_SIZE,))\n",
    "dec_embedding_input = Input(shape=(None, ))\n",
    "\n",
    "# Create the decoder layers.\n",
    "dec_embedding_layer = Embedding(output_dim=EMBEDDING_WIDTH,\n",
    "                                input_dim=MAX_WORDS,\n",
    "                                mask_zero=True)\n",
    "dec_layer1 = LSTM(LAYER_SIZE, return_state = True,\n",
    "                  return_sequences=True)\n",
    "dec_layer2 = LSTM(LAYER_SIZE, return_state = True,\n",
    "                  return_sequences=True)\n",
    "dec_layer3 = Dense(MAX_WORDS, activation='softmax')\n",
    "\n",
    "# Connect the decoder layers.\n",
    "dec_embedding_layer_outputs = dec_embedding_layer(\n",
    "    dec_embedding_input)\n",
    "dec_layer1_outputs, dec_layer1_state_h, dec_layer1_state_c = \\\n",
    "    dec_layer1(dec_embedding_layer_outputs,\n",
    "    initial_state=[dec_layer1_state_input_h,\n",
    "                   dec_layer1_state_input_c])\n",
    "dec_layer2_outputs, dec_layer2_state_h, dec_layer2_state_c = \\\n",
    "    dec_layer2(dec_layer1_outputs,\n",
    "    initial_state=[dec_layer2_state_input_h,\n",
    "                   dec_layer2_state_input_c])\n",
    "dec_layer3_outputs = dec_layer3(dec_layer2_outputs)\n",
    "\n",
    "# Build the model.\n",
    "dec_model = Model([dec_embedding_input,\n",
    "                   dec_layer1_state_input_h,\n",
    "                   dec_layer1_state_input_c,\n",
    "                   dec_layer2_state_input_h,\n",
    "                   dec_layer2_state_input_c],\n",
    "                  [dec_layer3_outputs, dec_layer1_state_h,\n",
    "                   dec_layer1_state_c, dec_layer2_state_h,\n",
    "                   dec_layer2_state_c])\n",
    "dec_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet connects the two models to build a full encoder-decoder network. We decided to use RMSProp as optimizer because some experiments indicate that it performs better than Adam for this specific model. We use sparse_categorical_crossentropy instead of the normal categorical_crossentropy as loss function because we have not one-hot encoded the output data.\n",
    "\n",
    "Even after connecting the encoder and decoder model to form a joint model, they can both still be used in isolation. If we train the joint model, it will update the weights of the first two models. This is useful because, when we do inference, we want an encoder model that is decoupled from the decoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile full training model.\n",
    "# We do not use the state output when training.\n",
    "train_enc_embedding_input = Input(shape=(None, ))\n",
    "train_dec_embedding_input = Input(shape=(None, ))\n",
    "intermediate_state = enc_model(train_enc_embedding_input)\n",
    "train_dec_output, _, _, _, _ = dec_model(\n",
    "    [train_dec_embedding_input] +\n",
    "    intermediate_state)\n",
    "training_model = Model([train_enc_embedding_input,\n",
    "                        train_dec_embedding_input],\n",
    "                        train_dec_output)\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "training_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=optimizer, metrics =['accuracy'])\n",
    "training_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final code snippet shows hos to train and test the model. We take a slightly different approach than in previous examples. In previous examples, we instructed fit() to train for multiple epochs, and then we studied the results and ended our program. In this example, we create our own training loop where we instruct fit() to train for only a single epoch at a time. We then use our model to create some predictions before going back and training for another epoch. This approach enables some detailed evaluation of just a small set of samples after each epoch.\n",
    "\n",
    "Most of the code sequence is the loop used to create translations for the smaller set of samples that we created from the test dataset. This piece of code consists of a loop that iterates over all the examples in sample_input_data. We provide the source sentence to the encoder model to create the resulting internal state and store to the variable last_states. We also initialize the variable prev_word_index with the index corresponding to the START symbol. We then enter the innermost loop and predict a single word using the decoder model. We also read out the internal state. This data is then used as input to the decoder model in the next iteration, and we iterate until the model produces a STOP token or until a given number of words have been produced. Finally, we convert the produced tokenized sequences into the corresponding word sequences and print them out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test repeatedly.\n",
    "for i in range(EPOCHS):\n",
    "    print('step: ' , i)\n",
    "    # Train model for one epoch.\n",
    "    history = training_model.fit(\n",
    "        [train_src_input_data, train_dest_input_data],\n",
    "        train_dest_target_data, validation_data=(\n",
    "            [test_src_input_data, test_dest_input_data],\n",
    "            test_dest_target_data), batch_size=BATCH_SIZE,\n",
    "        epochs=1)\n",
    "\n",
    "    # Loop through samples to see result\n",
    "    for (test_input, test_target) in zip(sample_input_data,\n",
    "                                         sample_target_data):\n",
    "        # Run a single sentence through encoder model.\n",
    "        x = np.reshape(test_input, (1, -1))\n",
    "        last_states = enc_model.predict(\n",
    "            x, verbose=0)\n",
    "        # Provide resulting state and START_INDEX as input\n",
    "        # to decoder model.\n",
    "        prev_word_index = START_INDEX\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        for j in range(MAX_LENGTH):\n",
    "            x = np.reshape(np.array(prev_word_index), (1, 1))\n",
    "            # Predict next word and capture internal state.\n",
    "            preds, dec_layer1_state_h, dec_layer1_state_c, \\\n",
    "                dec_layer2_state_h, dec_layer2_state_c = \\\n",
    "                    dec_model.predict(\n",
    "                        [x] + last_states, verbose=0)\n",
    "            last_states = [dec_layer1_state_h,\n",
    "                           dec_layer1_state_c,\n",
    "                           dec_layer2_state_h,\n",
    "                           dec_layer2_state_c]\n",
    "            # Find the most probable word.\n",
    "            prev_word_index = np.asarray(preds[0][0]).argmax()\n",
    "            pred_seq.append(prev_word_index)\n",
    "            if prev_word_index == STOP_INDEX:\n",
    "                break\n",
    "        tokens_to_words(src_tokenizer, test_input)\n",
    "        tokens_to_words(dest_tokenizer, test_target)\n",
    "        tokens_to_words(dest_tokenizer, pred_seq)\n",
    "        print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
