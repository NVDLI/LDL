{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example generates feature vectors corresponding to images in the input dataset. These feature vectors are used by the image captioning network in the code example v9_5_image_captioning. More context for this code example can be found in video 9.5 \"Programming Example: Image Captioning with TensorFlow\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614). This is notebook 1 of 2 for this example.\n",
    "\n",
    "This programming example assumes that the following resources from the COCO dataset are available:\n",
    "The file captions_train2014.json should be located in the directory ../data/coco/\n",
    "All the training images should be located in the directory ../data/coco/train2014/\n",
    "\n",
    "The resulting feature vectors will be stored in the directory tf_data/feature_vectors/\n",
    "\n",
    "The import statements are shown in the first code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import \\\n",
    "    preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import pickle\n",
    "import gzip\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "TRAINING_FILE_DIR = '../data/coco/'\n",
    "OUTPUT_FILE_DIR = 'tf_data/feature_vectors/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parts of the dataset that we will use are contained in two resources. The first resource is a json file that contains captions as well as filenames and some other information for the images. We make the assumption that you have placed that file in the directory pointed to by the variable TRAINING_FILE_DIR. The images themselves are stored as individual image files and are assumed to be located in a directory named train2014 in the directory pointed to by TRAINING_FILE_DIR. The COCO dataset contains elaborate tools to parse and read the rich information about the various images, but because we are only interested in the image captions, we choose to directly access the json file and extract the limited data that we need ourselves. The code snippet below opens the json file and creates a dictionary that, for each image, maps a unique key to a list of strings. The first string in each list represents the image filename, and the subsequent strings are alternative captions for the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_FILE_DIR \\\n",
    "          + 'captions_train2014.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "image_dict = {}\n",
    "for image in data['images']:\n",
    "    image_dict[image['id']] = [image['file_name']]\n",
    "for anno in data['annotations']:\n",
    "    image_dict[anno['image_id']].append(anno['caption'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create our pretrained VGG19 model, which is done in the next code snippet. We first obtain the full VGG19 model with weights trained from the ImageNet dataset. We then create a new model (model_new) from that model by stating that we want to use the layer named block5_conv4 as output. A fair question is how we figured out that name. As you can see in the code snippet, we first printed out the summary of the full VGG19 model. This summary includes the layer names, and we saw that the last convolutional layer was named block5_conv4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network without top layers.\n",
    "model = VGG19(weights='imagenet')\n",
    "model.summary()\n",
    "model_new = Model(inputs=model.input,\n",
    "                  outputs=model.get_layer('block5_conv4').output)\n",
    "model_new.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run all the images through the network and extract the feature vectors and save to disk. This is done by the code snippet below. We traverse the dictionary to obtain the image file names. Every loop iteration does the processing for a single image and saves the feature vectors for that one image in a single file. Before running the image through the network, we perform some preprocessing. The image sizes in the COCO dataset vary from image to image, so we first read the file to determine its file size. We determine the aspect ratio and then reread the image scaled to a size at which the shortest side ends up being 256 pixels. We then crop the center 224Ã—224 region of the resulting image to end up with the input dimensions that our VGG19 network expects. We finally run the VGG19 preprocessing function, which standardizes the data values in the image before we run the image through the network. The output of the network will be an array with the shape (1, 14, 14, 512) representing the results from a batch of images where the first dimension indicates that the batch size is 1. Therefore, we extract the first (and only) element from this array (y[0]) and save it as a gzipped pickle file with the same name as the image but with the extension .pickle.gz in the directory feature_vectors. When we have looped through all images, we also save the dictionary file as caption_file. pickle.gz so we do not need to parse the json file again later in the code that does the actual training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all images through the network and save the output.\n",
    "for i, key in enumerate(image_dict.keys()):\n",
    "    if i % 1000 == 0:\n",
    "        print('Progress: ' + str(i) + ' images processed')\n",
    "    item = image_dict.get(key)\n",
    "    filename = TRAINING_FILE_DIR + 'train2014/' + item[0]\n",
    "\n",
    "    # Determine dimensions.\n",
    "    image = load_img(filename)\n",
    "    width = image.size[0]\n",
    "    height = image.size[1]\n",
    "\n",
    "    # Resize so shortest side is 256 pixels.\n",
    "    if height > width:\n",
    "        image = load_img(filename, target_size=(\n",
    "            int(height/width*256), 256))\n",
    "    else:\n",
    "        image = load_img(filename, target_size=(\n",
    "            256, int(width/height*256)))\n",
    "    width = image.size[0]\n",
    "    height = image.size[1]\n",
    "    image_np = img_to_array(image)\n",
    "\n",
    "    # Crop to center 224x224 region.\n",
    "    h_start = int((height-224)/2)\n",
    "    w_start = int((width-224)/2)\n",
    "    image_np = image_np[h_start:h_start+224,\n",
    "                        w_start:w_start+224]\n",
    "\n",
    "    # Rearrange array to have one more\n",
    "    # dimension representing batch size = 1.\n",
    "    image_np = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    # Call model and save resulting tensor to disk.\n",
    "    X = preprocess_input(image_np)\n",
    "    y = model_new.predict(X)\n",
    "    save_filename = OUTPUT_FILE_DIR + \\\n",
    "        item[0] + '.pickle.gzip'\n",
    "    pickle_file = gzip.open(save_filename, 'wb')\n",
    "    pickle.dump(y[0], pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "# Save the dictionary containing captions and filenames.\n",
    "save_filename = OUTPUT_FILE_DIR + 'caption_file.pickle.gz'\n",
    "pickle_file = gzip.open(save_filename, 'wb')\n",
    "pickle.dump(image_dict, pickle_file)\n",
    "pickle_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2130_py3100)",
   "language": "python",
   "name": "tf2130_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
