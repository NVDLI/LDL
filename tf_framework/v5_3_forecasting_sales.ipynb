{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to use a recurrent neural network to solve a time series prediction problem. The goal is to predict future sales data based on historical values. More context for this code example can be found in video 5.3 \"Programming Example: Forecasting Book Sales with TensorFlow\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with initialization code. First, we import modules that we need for the network. We also load the data file into an array. We then split the data into training data (the first 80% of the data points) and test data (the remaining 20% of the months). The data is assumed to be in the file ../data/book_store_sales.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "MIN = 12\n",
    "FILE_NAME = '../data/book_store_sales.csv'\n",
    "\n",
    "def readfile(file_name):\n",
    "    file = open(file_name, 'r', encoding='utf-8')\n",
    "    next(file)\n",
    "    data = []\n",
    "    for line in (file):\n",
    "        values = line.split(',')\n",
    "        data.append(float(values[1]))\n",
    "    file.close()\n",
    "    return np.array(data, dtype=np.float32)\n",
    "\n",
    "# Read data and split into training and test data.\n",
    "sales = readfile(FILE_NAME)\n",
    "months = len(sales)\n",
    "split = int(months * TRAIN_TEST_SPLIT)\n",
    "train_sales = sales[0:split]\n",
    "test_sales = sales[split:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet plots the historical sales data. The data shows a clear seasonal pattern along with an indication that the overall trend in sales has changed over time, presumably due to increased online sales. The data starts in 1992 and ends in March 2020. The drop for the last month was likely caused by the COVID-19 pandemic hitting the United States.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset\n",
    "x = range(len(sales))\n",
    "plt.plot(x, sales, 'r-', label='book sales')\n",
    "plt.title('Book store sales')\n",
    "plt.axis([0, 339, 0.0, 3000.0])\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Sales (millions $)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purposes, create output corresponding to a naive model that predicts that the sales next month will be the same as the sales this month. Compare this to the correct data by plotting the values side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot naive prediction\n",
    "test_output = test_sales[MIN:]\n",
    "naive_prediction = test_sales[MIN-1:-1]\n",
    "x = range(len(test_output))\n",
    "plt.plot(x, test_output, 'g-', label='test_output')\n",
    "plt.plot(x, naive_prediction, 'm-', label='naive prediction')\n",
    "plt.title('Book store sales')\n",
    "plt.axis([0, len(test_output), 0.0, 3000.0])\n",
    "plt.xlabel('months')\n",
    "plt.ylabel('Monthly book store sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to standardize the data points by subtracting the mean and dividing by the standard deviation of the training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize train and test data.\n",
    "# Use only training seasons to compute mean and stddev.\n",
    "mean = np.mean(train_sales)\n",
    "stddev = np.std(train_sales)\n",
    "train_sales_std = (train_sales - mean)/stddev\n",
    "test_sales_std = (test_sales - mean)/stddev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous examples, the datasets were already organized into individual examples. For example, we had an array of images serving as input values and an associated array of classes serving as expected output values. However, the data that we created in this example is raw historical data and not yet organized as a set of training and test examples. This is the next step in our code example. The code snippet below allocates tensors for the training data and initializes all entries to 0. It then loops through the historical data and creates training examples, then does the same thing with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples.\n",
    "train_months = len(train_sales)\n",
    "train_X = np.zeros((train_months-MIN, train_months-1, 1))\n",
    "train_y = np.zeros((train_months-MIN, 1))\n",
    "for i in range(0, train_months-MIN):\n",
    "    train_X[i, -(i+MIN):, 0] = train_sales_std[0:i+MIN]\n",
    "    train_y[i, 0] = train_sales_std[i+MIN]\n",
    "\n",
    "# Create test examples.\n",
    "test_months = len(test_sales)\n",
    "test_X = np.zeros((test_months-MIN, test_months-1, 1))\n",
    "test_y = np.zeros((test_months-MIN, 1))\n",
    "for i in range(0, test_months-MIN):\n",
    "    test_X[i, -(i+MIN):, 0] = test_sales_std[0:i+MIN]\n",
    "    test_y[i, 0] = test_sales_std[i+MIN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to define and train our network. This is shown in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, activation='relu',\n",
    "                    input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam',\n",
    "              metrics =['mean_absolute_error'])\n",
    "model.summary()\n",
    "history = model.fit(train_X, train_y,\n",
    "                    validation_data\n",
    "                    = (test_X, test_y), epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE, verbose=2,\n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, create naive predictions based on standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create naive prediction based on standardized data.\n",
    "test_output = test_sales_std[MIN:]\n",
    "naive_prediction = test_sales_std[MIN-1:-1]\n",
    "mean_squared_error = np.mean(np.square(naive_prediction\n",
    "                                       - test_output))\n",
    "mean_abs_error = np.mean(np.abs(naive_prediction\n",
    "                                - test_output))\n",
    "print('naive test mse: ', mean_squared_error)\n",
    "print('naive test mean abs: ', mean_abs_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shed some light on how this affects the end behavior, let us use our newly trained model to do some predictions and then plot these predictions next to the actual values. The code snippet below demonstrates how this can be done. We first call model.predict with the test input as argument. The second argument is the batch size, and we state the length of the input tensor as the batch size (i.e., we ask it to do a prediction for all the input examples in parallel). During training, the batch size will affect the result, but for prediction, it should not affect anything except for possibly runtime. We could just as well have used 16 or 32 or some other value. The model will return a 2D array with the output values. Because each output value is a single value, a 1D array works just as well, and that is the format we want in order to enable plotting the data, so we call np.reshape to change the dimensions of the array. The network works with standardized data, so the output will not represent demand directly. We must first destandardize the data by doing the reverse operation compared to the standardization. That is, we multiply by the standard deviation and add the mean. We then plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model to predict the test data\n",
    "predicted_test = model.predict(test_X, len(test_X))\n",
    "predicted_test = np.reshape(predicted_test,\n",
    "                            (len(predicted_test)))\n",
    "predicted_test = predicted_test * stddev + mean\n",
    "\n",
    "# Plot test prediction.\n",
    "x = range(len(test_sales)-MIN)\n",
    "plt.plot(x, predicted_test, 'm-',\n",
    "         label='predicted test_output')\n",
    "plt.plot(x, test_sales[-(len(test_sales)-MIN):],\n",
    "         'g-', label='actual test_output')\n",
    "plt.title('Book sales')\n",
    "plt.axis([0, 55, 0.0, 3000.0])\n",
    "plt.xlabel('months')\n",
    "plt.ylabel('Predicted book sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2130_py3100)",
   "language": "python",
   "name": "tf2130_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
