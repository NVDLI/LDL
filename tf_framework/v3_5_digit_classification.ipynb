{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example contains a TensorFlow implementation of a network classifying handwritten digits. More context for this code example can be found in video 3.5 \"Programming Example: Digit Classification with TensorFlow\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code snippet contains initialization code for our TensorFlow example. TensorFlow has its own random seed that needs to be set if we want reproducible results. The code snippet also sets the logging level to only print out errors while suppressing warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load and prepare our MNIST dataset. Because MNIST is a common dataset, it is included in TensorFlow/Keras. We can access it by a call to keras.datasets.mnist and load_data. The variables train_images and test_images will contain the input values, and the variables train_labels and test_labels will contain the ground truth.\n",
    "\n",
    "We standardize the input data and one-hot encode the labels. We use the function to_categorical to one-hot encode our labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test datasets.\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images,\n",
    "                               test_labels) = mnist.load_data()\n",
    "\n",
    "# Standardize the data.\n",
    "mean = np.mean(train_images)\n",
    "stddev = np.std(train_images)\n",
    "train_images = (train_images - mean) / stddev\n",
    "test_images = (test_images - mean) / stddev\n",
    "\n",
    "# One-hot encode labels.\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create our network. There is no need to define variables for individual neurons because the framework provides functionality to instantiate entire layers of neurons at once. We do need to decide how to initialize the weights, which we do by creating an initializer object, as shown in the code snippet below.\n",
    "\n",
    "The network is created by instantiating a keras.Sequential object, which implies that we are using the Keras Sequential API. We pass a list of layers as an argument to the Sequential class. The first layer is a Flatten layer, which does not do computations but only changes the organization of the input. In our case, the inputs are changed from a 28×28 array into an array of 784 elements. The second and third layers are both Dense layers, which means they are fully connected. The first argument tells how many neurons each layer should have, and the activation argument tells the type of activation function; we choose tanh and sigmoid, where sigmoid means the logistic sigmoid function. We pass our initializer object to initialize the regular weights using the kernel_initializer argument. The bias weights are initialized to 0 using the bias_initializer argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object used to initialize weights.\n",
    "initializer = keras.initializers.RandomUniform(\n",
    "    minval=-0.1, maxval=0.1)\n",
    "\n",
    "# Create a Sequential model.\n",
    "# 784 inputs.\n",
    "# Two Dense (fully connected) layers with 25 and 10 neurons.\n",
    "# tanh as activation function for hidden layer.\n",
    "# Logistic (sigmoid) as activation function for output layer.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(25, activation='tanh',\n",
    "                       kernel_initializer=initializer,\n",
    "                       bias_initializer='zeros'),\n",
    "    keras.layers.Dense(10, activation='sigmoid',\n",
    "                       kernel_initializer=initializer,\n",
    "                       bias_initializer='zeros')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the network, which is done in the next code snippet. We first create a keras.optimizer.SGD object. This means that we want to use stochastic gradient descent (SGD) when training the network. We then prepare the model for training by calling the model’s compile function. We provide parameters to specify which loss function to use, the optimizer that we just created and that we are interested in looking at the accuracy metric during training. Note that although this example uses mean squared error (MSE) as loss function, in general that is not a good choice in combination with sigmoid output neurons. We change this in subsequent programming examples.\n",
    "\n",
    "We finally call the fit function for the model, which starts the training process. As the function name indicates, it fits the model to the data. The first two arguments specify the training dataset. The parameter validation_data is the test dataset. Our variables EPOCHS and BATCH_SIZE from the initialization code determine how many epochs to train for and what batch size we use. We had set BATCH_SIZE to 1, which means that we update the weight after a single training example. We set verbose=2 to get a reasonable amount of information printed during the training process and set shuffle to True to indicate that we want the order of the training data to be randomized during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stochastic gradient descent (SGD) with\n",
    "# learning rate of 0.01 and no other bells and whistles.\n",
    "# MSE as loss function and report accuracy during training.\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer = opt,\n",
    "              metrics =['accuracy'])\n",
    "# Train the model for 20 epochs.\n",
    "# Shuffle (randomize) order.\n",
    "# Update weights after each example (batch_size=1).\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    verbose=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2130_py3100)",
   "language": "python",
   "name": "tf2130_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
