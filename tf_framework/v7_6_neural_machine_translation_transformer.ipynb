{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to build a neural machine translation network. It is a sequence-to-sequence network based on a Transformer encoder-decoder architecture. More context for this code example can be found in video 7.6 \"Programming Example: Machine Translation Using Transformer with TensorFlow\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n",
    "\n",
    "The data used to train the model is expected to be in the file ../data/fra.txt.\n",
    "We begin by importing modules that we need for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence \\\n",
    "    import pad_sequences\n",
    "from keras_nlp.layers import TransformerEncoder\n",
    "from keras_nlp.layers import TransformerDecoder\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some constants. We specify a vocabulary size of 10,000 symbols, out of which four indices are reserved for padding, out-of-vocabulary words (denoted as UNK), START tokens, and STOP tokens. Our training corpus is large, so we set the parameter READ_LINES to the number of lines in the input file we want to use in our example (60,000). The parameter LAYER_SIZE defines the width of the intermediate fully-connected layer in the Transformer, and the embedding layers output 128 dimensions (EMBEDDING_WIDTH). We use 20% (TEST_PERCENT) of the dataset as test set and further select 20 sentences (SAMPLE_SIZE) to inspect in detail during training. We limit the length of the source and destination sentences to, at most, 60 words (MAX_LENGTH). Finally, we provide the path to the data file, where each line is expected to contain two versions of the same sentence (one in each language) separated by a tab character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_LINES = 60000\n",
    "NUM_HEADS = 8\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128\n",
    "TEST_PERCENT = 0.2\n",
    "SAMPLE_SIZE = 20\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "SRC_DEST_FILE_NAME = '../data/fra.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the function used to read the input data file and do some initial processing. Each line is split into two strings, where the first contains the sentence in the destination language and the second contains the sentence in the source language. We use the function text_to_word_sequence() to clean the data somewhat (make everything lowercase and remove punctuation) and split each sentence into a list of individual words. If the list (sentence) is longer than the maximum allowed length, then it is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read file.\n",
    "def read_file_combined(file_name, max_len):\n",
    "    file = open(file_name, 'r', encoding='utf-8')\n",
    "    src_word_sequences = []\n",
    "    dest_word_sequences = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i == READ_LINES:\n",
    "            break\n",
    "        pair = line.split('\\t')\n",
    "        word_sequence = text_to_word_sequence(pair[1])\n",
    "        src_word_sequence = word_sequence[0:max_len]\n",
    "        src_word_sequences.append(src_word_sequence)\n",
    "        word_sequence = text_to_word_sequence(pair[0])\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    file.close()\n",
    "    return src_word_sequences, dest_word_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows functions used to turn sequences of words into\n",
    "sequences of tokens, and vice versa. We call tokenize() a single time for each\n",
    "language, so the argument sequences is a list of lists where each of the inner\n",
    "lists represents a sentence. The Tokenizer class assigns indices to the most\n",
    "common words and returns either these indices or the reserved OOV_INDEX\n",
    "for less common words that did not make it into the vocabulary. We tell the\n",
    "Tokenizer to use a vocabulary of 9998 (MAX_WORDS-2)â€”that is, use only\n",
    "indices 0 to 9997, so that we can use indices 9998 and 9999 as our START and\n",
    "STOP tokens (the Tokenizer does not support the notion of START and STOP\n",
    "tokens but does reserve index 0 to use as a padding token and index 1 for outof-\n",
    "vocabulary words). Our tokenize() function returns both the tokenized\n",
    "sequence and the Tokenizer object itself. This object will be needed anytime we\n",
    "want to convert tokens back into words.\n",
    "\n",
    "The function tokens_to_words() requires a Tokenizer and a list of indices. We simply check for the reserved indices: If we find a match, we replace them with hardcoded strings, and if we find no match, we let the Tokenizer convert the index to the corresponding word string. The Tokenizer expects a list of lists of indices and returns a list of strings, which is why we need to call it with [[index]] and then select the 0th element to arrive at a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to tokenize and un-tokenize sequences.\n",
    "def tokenize(sequences):\n",
    "    # \"MAX_WORDS-2\" used to reserve two indices\n",
    "    # for START and STOP.\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
    "                          oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts(\n",
    "                [[index]])[0])\n",
    "    print(word_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these helper functions, it is trivial to read the input data\n",
    "file and convert into tokenized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and tokenize.\n",
    "src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME,\n",
    "                                       MAX_LENGTH)\n",
    "src_tokenizer, src_token_seq = tokenize(src_seq)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to arrange the data into tensors that can be used for training and testing. The following example provides some insight into what we need as input and output for a single training example, where src_input is the input to the encoder network, dest_input is the input to the decoder network, and dest_target is the desired output from the decoder network:\n",
    "\n",
    "src_input = [PAD, PAD, PAD, id(\"je\"), id(\"suis\"), id(\"Ã©tudiant\")]\n",
    "dest_input = [START, id(\"i\"), id(\"am\"), id(\"a\"), id(\"student\"), STOP, PAD, PAD]\n",
    "dest_target = [one_hot_id(\"i\"), one_hot_id(\"am\"), one_hot_id(\"a\"), one_hot_id(\"student\"), one_hot_id(STOP), one_hot_id(PAD), one_hot_id(PAD), one_hot_id(PAD)]\n",
    "\n",
    "In the example, id(string) refers to the tokenized index of the string, and one_hot_id is the one-hot encoded version of the index. We have assumed that the longest source sentence is six words, so we padded src_input to be of that length. Similarly, we have assumed that the longest destination sentence is eight words including START and STOP tokens, so we padded both dest_input and dest_target to be of that length. Note how the symbols in dest_input are offset by one location compared to the symbols in dest_target because when we later do inference, the inputs into the decoder network will be coming from the output of the network for the previous timestep. Although this example has shown the training example as being lists, in reality, they will be rows in NumPy arrays, where each array contains multiple training examples.\n",
    "\n",
    "The padding is done to ensure that we can use mini-batches for training. That is, all source sentences need to be the same length, and all destination sentences need to be the same length. We pad the source input at the beginning (known as prepadding) and the destination at the end (known as postpadding).\n",
    "\n",
    "The code snippet below shows a compact way of creating the three arrays that we need. The first two lines create two new lists, each containing the destination sequences but the first (dest_target_token_seq) also augmented with STOP_INDEX after each sequence and the second (dest_input_token_seq) augmented with both START_INDEX and STOP_INDEX. It is easy to miss that dest_input_token_seq has a STOP_INDEX, but that falls out naturally because it is created from the dest_target_token_seq for which a STOP_INDEX was just added to each sentence.\n",
    "\n",
    "Next, we call pad_sequences() on both the original src_input_data list (of lists) and on these two new destination lists. The pad_sequences() function pads the sequences with the PAD value and then returns a NumPy array. The default behavior of pad_sequences is to do prepadding, and we do that for the source sequence but explicitly ask for postpadding for the destination sequences.\n",
    "\n",
    "You might wonder why there is no call to to_categorical() in the statement that creates the target (output) data. We are used to wanting to have the ground truth one-hot encoded for textual data. Not doing so is an optimization to avoid wasting too much memory. With a vocabulary of 10,000 words, and 60,000 training examples, where each training example is a sentence, the memory footprint of the one-hot encoded data starts becoming a problem. Therefore, instead of one-hot encoding all data up front, there is a way to let Keras deal with that in the loss function itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in\n",
    "                        dest_target_token_seq]\n",
    "src_input_data = pad_sequences(src_token_seq)\n",
    "dest_input_data = pad_sequences(dest_input_token_seq,\n",
    "                                padding='post')\n",
    "dest_target_data = pad_sequences(\n",
    "    dest_target_token_seq, padding='post', maxlen\n",
    "    = len(dest_input_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet demonstrates how we can manually split our dataset into a training dataset and a test dataset. In previous examples, we either relied on datasets that are already split this way or we used functionality inside of Keras when calling the fit() function. However, in this case, we want some more control ourselves because we will want to inspect a few select members of the test set in detail. We split the dataset by first creating a list test_indices, which contains a 20% (TEST_PERCENT) subset of all the numbers from 0 to Nâˆ’1, where N is the size of our original dataset. We then create a list train_indices, which contains the remaining 80%. We can now use these lists to select a number of rows in the matrices representing the dataset and create two new collections of matrices, one to be used as training set and one to be used as test set. Finally, we create a third collection of matrices, which only contains 20 (SAMPLE_SIZE) random examples from the test dataset. We will use them to inspect the resulting translations in detail, but since that is a manual process, we limit ourselves to a small number of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set.\n",
    "rows = len(src_input_data[:,0])\n",
    "all_indices = list(range(rows))\n",
    "test_rows = int(rows * TEST_PERCENT)\n",
    "test_indices = random.sample(all_indices, test_rows)\n",
    "train_indices = [x for x in all_indices if x not in test_indices]\n",
    "\n",
    "train_src_input_data = src_input_data[train_indices]\n",
    "train_dest_input_data = dest_input_data[train_indices]\n",
    "train_dest_target_data = dest_target_data[train_indices]\n",
    "\n",
    "test_src_input_data = src_input_data[test_indices]\n",
    "test_dest_input_data = dest_input_data[test_indices]\n",
    "test_dest_target_data = dest_target_data[test_indices]\n",
    "\n",
    "# Create a sample of the test set that we will inspect in detail.\n",
    "test_indices = list(range(test_rows))\n",
    "sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n",
    "sample_input_data = test_src_input_data[sample_indices]\n",
    "sample_target_data = test_dest_target_data[sample_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide ordering information between the embeddings we need to add positional encodings to each embedding. We do this by creating a class PositionalEmbedding that extends the Embedding class. We calculate the positional encoding using sine and cosine as in the original Transformer paper and add it to the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(Embedding):\n",
    "    def __init__(self, max_len, *args, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(*args, **kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "    def create_positional_encodings(self):\n",
    "        i_range = np.arange(self.output_dim).reshape(1, self.output_dim)\n",
    "        pos_range = np.arange(self.max_len).reshape(self.max_len, 1)\n",
    "        sine_matrix = np.sin(1 / np.power(10000, i_range/self.output_dim) * pos_range)\n",
    "        cosine_matrix = np.cos(1 / np.power(10000, (i_range-1)/self.output_dim) * pos_range)\n",
    "        pos_matrix = np.zeros((self.max_len, self.output_dim))\n",
    "        for i in range(self.output_dim):\n",
    "            if (i % 2 == 0):\n",
    "                pos_matrix[:, i] = sine_matrix[:, i]\n",
    "            else:\n",
    "                pos_matrix[:, i] = cosine_matrix[:, i]\n",
    "        pos_matrix = pos_matrix.reshape(1, self.max_len, self.output_dim)\n",
    "        return tf.cast(pos_matrix, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings = super(PositionalEmbedding, self).call(inputs)\n",
    "        embeddings = embeddings * math.sqrt(EMBEDDING_WIDTH)\n",
    "        length = tf.shape(inputs)[1]\n",
    "        pos_encodings = self.positional_encodings[:, :length, :]\n",
    "        return embeddings + pos_encodings\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build our model. It consists of an encoder part and a decoder part. The encoder consists of a PositionalEmbedding layer and two Transformer encoder modules stacked on top of each other. The decoder consists of a PositionalEmbedding layer, two Transformer decoder modules stacked on top of each other, and a fully connected softmax layer. We define the encoder and decoder as two separate models, which we later tie together. To be able to express this complex model we need to use the Keras Functional API.\n",
    "\n",
    "The code snippet below contains the implementation of the encoder model. We first define the layers and then connect them together. Once all layers are connected, we create the actual model by calling the Model() constructor and providing arguments to specify what inputs and outputs will be external to the model. The model takes the source sentence as input and produces the encoded intermediate representation as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder model.\n",
    "# Input is input sequence in source language.\n",
    "enc_embedding_input = Input(shape=(None, ))\n",
    "\n",
    "# Create the encoder layers.\n",
    "enc_embedding_layer = PositionalEmbedding(max_len=MAX_LENGTH, input_dim=MAX_WORDS,\n",
    "                                          output_dim=EMBEDDING_WIDTH, mask_zero=True)\n",
    "enc_layer1 = TransformerEncoder(intermediate_dim=LAYER_SIZE, num_heads=NUM_HEADS,\n",
    "                                dropout=0.1)\n",
    "enc_layer2 = TransformerEncoder(intermediate_dim=LAYER_SIZE, num_heads=NUM_HEADS,\n",
    "                                dropout=0.1)\n",
    "\n",
    "# Connect the encoder layers.\n",
    "enc_embedding_layer_outputs = \\\n",
    "    enc_embedding_layer(enc_embedding_input)\n",
    "\n",
    "\n",
    "enc_layer1_outputs = enc_layer1(enc_embedding_layer_outputs)\n",
    "enc_layer2_outputs = enc_layer2(enc_layer1_outputs)\n",
    "\n",
    "# Build the model.\n",
    "enc_model = Model(enc_embedding_input, enc_layer2_outputs)\n",
    "enc_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the implementation of the decoder model. The first Transformer decoder module takes the output from the embedding layer as one input (for self-attention), and also takes the output from the encoder stack as a second output (for cross-attention). Similarly, the second Transformer decoder module takes the output from the first Transformer decoder module as well as the output from the encoder stack as inputs. The model ends with a fully-connected softmax layer.\n",
    "\n",
    "We create the model by calling the Model() constructor. The inputs consist of the destination sentence (time shifted by one timestep) and output from the encoder stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decoder model.\n",
    "# Input to the network is input sequence in destination\n",
    "# language and state from encoder.\n",
    "dec_state_input = Input(shape=(None, EMBEDDING_WIDTH),)\n",
    "dec_embedding_input = Input(shape=(None,))\n",
    "\n",
    "# Create the encoder layers.\n",
    "dec_embedding_layer = PositionalEmbedding(max_len=MAX_LENGTH, input_dim=MAX_WORDS,\n",
    "                                          output_dim=EMBEDDING_WIDTH, mask_zero=True)\n",
    "dec_layer1 = TransformerDecoder(intermediate_dim=LAYER_SIZE, num_heads=NUM_HEADS,\n",
    "                                dropout=0.1)\n",
    "dec_layer2 = TransformerDecoder(intermediate_dim=LAYER_SIZE, num_heads=NUM_HEADS,\n",
    "                                dropout=0.1)\n",
    "dec_layer3 = Dense(MAX_WORDS, activation='softmax')\n",
    "\n",
    "# Connect the decoder layers.\n",
    "dec_embedding_layer_outputs = dec_embedding_layer(\n",
    "    dec_embedding_input)\n",
    "\n",
    "dec_layer1_outputs = dec_layer1(dec_embedding_layer_outputs,\n",
    "                                dec_state_input)\n",
    "dec_layer2_outputs = dec_layer2(dec_layer1_outputs,\n",
    "                                dec_state_input)\n",
    "dec_layer3_outputs = dec_layer3(dec_layer2_outputs)\n",
    "\n",
    "# Build the model.\n",
    "dec_model = Model([dec_embedding_input,\n",
    "                   dec_state_input],\n",
    "                   dec_layer3_outputs)\n",
    "dec_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet connects the two models to build a full encoder-decoder network. We decided to use RMSProp as optimizer because some experiments indicate that it performs better than Adam for this specific model. We use sparse_categorical_crossentropy instead of the normal categorical_crossentropy as loss function because we have not one-hot encoded the output data.\n",
    "\n",
    "Even after connecting the encoder and decoder model to form a joint model, they can both still be used in isolation. If we train the joint model, it will update the weights of the first two models. This is useful because, when we do inference, we want an encoder model that is decoupled from the decoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile full training model.\n",
    "train_enc_embedding_input = Input(shape=(None, ))\n",
    "train_dec_embedding_input = Input(shape=(None, ))\n",
    "intermediate_state = enc_model(train_enc_embedding_input)\n",
    "train_dec_output = dec_model([train_dec_embedding_input,\n",
    "                             intermediate_state])\n",
    "training_model = Model([train_enc_embedding_input,\n",
    "                        train_dec_embedding_input],\n",
    "                        train_dec_output)\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "training_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=optimizer, metrics =['accuracy'])\n",
    "training_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final code snippet shows hos to train and test the model. We create our own training loop where we instruct fit() to train for only a single epoch at a time. We then use our model to create some predictions before going back and training for another epoch. This approach enables some detailed evaluation of just a small set of samples after each epoch.\n",
    "\n",
    "Most of the code sequence is the loop used to create translations for the smaller set of samples that we created from the test dataset. This piece of code consists of a loop that iterates over all the examples in sample_input_data. We provide the source sentence to the encoder model to create the resulting internal state and store to the variable intermediate_states. We then set the input x to the START token and use the decoder to make a prediction. We retrieve the most probable word and append it to x. We then provide this sequence to the decoder and make a new prediction. We iterate this with a gradually growing input sequence in an autoregressive manner until the model produces a STOP token or until a given number of words have been produced. Finally, we convert the produced tokenized sequences into the corresponding word sequences and print them out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test repeatedly.\n",
    "for i in range(EPOCHS):\n",
    "    print('step: ' , i)\n",
    "    # Train model for one epoch.\n",
    "    history = training_model.fit(\n",
    "        [train_src_input_data, train_dest_input_data],\n",
    "        train_dest_target_data, validation_data=(\n",
    "            [test_src_input_data, test_dest_input_data],\n",
    "            test_dest_target_data), batch_size=BATCH_SIZE,\n",
    "        epochs=1)\n",
    "\n",
    "    # Loop through samples to see result\n",
    "    for (test_input, test_target) in zip(sample_input_data,\n",
    "                                         sample_target_data):\n",
    "        # Run a single sentence through encoder model.\n",
    "        x = np.reshape(test_input, (1, -1))\n",
    "        intermediate_states = enc_model.predict(\n",
    "            x, verbose=0)\n",
    "        # Provide resulting state and START_INDEX as input\n",
    "        # to decoder model.\n",
    "        x = np.array([[START_INDEX]])\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        \n",
    "        for j in range(MAX_LENGTH):\n",
    "            # Predict next word and capture internal state.\n",
    "            preds = dec_model.predict(\n",
    "                [x, intermediate_states], verbose=0)\n",
    "            # Find the most probable word.\n",
    "            word_index = np.asarray(preds[0][j]).argmax()\n",
    "            pred_seq.append(word_index)\n",
    "            if word_index == STOP_INDEX:\n",
    "                break\n",
    "            x = np.append(x, [[word_index]], axis=1)\n",
    "        tokens_to_words(src_tokenizer, test_input)\n",
    "        tokens_to_words(dest_tokenizer, test_target)\n",
    "        tokens_to_words(dest_tokenizer, pred_seq)\n",
    "        print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2130_py3100)",
   "language": "python",
   "name": "tf2130_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
