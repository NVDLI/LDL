{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example trains an encoder-decoder network to generate textual descriptions of images. The architecture resembles the language translation network in c14e1_seq2seq2_translate, but the encoder is a convolutional network instead of a recurrent network. Additionally, the decoder makes use of attention. The implementation relies on precomputed feature vectors from c16e1_create_vectors. More context for this code example can be found in the section \"Programming Example: Attention-Based Image Captioning\" in Chapter 16 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n",
    "\n",
    "We start with import statements in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import \\\n",
    "    text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import \\\n",
    "    pad_sequences\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization statements for our program are found below. They are similar to what we used in the language translation example, but some of the lines deserve further attention. The variable READ_IMAGES can be used to limit the number of images that we use for training. We set it to 90,000, which is more than the total number of images we have. You can decrease it if necessary (e.g., if you run into memory limits of your machine). We also provide the paths to four files that we will use as test images. You can replace those to point to images of your own choice when you run this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_IMAGES = 90000\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "TRAINING_FILE_DIR = 'pt_data/feature_vectors/'\n",
    "TEST_FILE_DIR = '../data/test_images/'\n",
    "TEST_IMAGES = ['boat.jpg',\n",
    "               'cat.jpg',\n",
    "               'table.jpg',\n",
    "               'bird.jpg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the functions we use to read the image captions. The function to read the captions reads the pickled directory file that we previously prepared. From this, we create a list image_paths with the filenames for the feature vectors and one list, dest_word_sequences, which contains the first image caption for each image. To keep things simple, we simply discard the alternative captions for each image.\n",
    "\n",
    "The list dest_word_sequences is equivalent to the destination language sentence in the language translation example. This function does not load all the feature vectors but just the paths to them. The reason for this is that the feature vectors for all the images consume a fair amount of space, so for many machines, it would be impractical to hold the entire dataset in memory during training. Instead, we read the feature vectors on the fly when they are needed. This is a common technique when working with large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read file.\n",
    "def read_training_file(file_name, max_len):\n",
    "    pickle_file = gzip.open(file_name, 'rb')\n",
    "    image_dict = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    image_paths = []\n",
    "    dest_word_sequences = []\n",
    "    for i, key in enumerate(image_dict):\n",
    "        if i == READ_IMAGES:\n",
    "            break\n",
    "        image_item = image_dict[key]\n",
    "        image_paths.append(image_item[0])\n",
    "        caption = image_item[1]\n",
    "        word_sequence = text_to_word_sequence(caption)\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    return image_paths, dest_word_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet contains functions to tokenize and untokenize the sentences. These are similar, if not identical, to what we used in c14e1_seq2seq_translate. We finally call the functions to read and tokenize the image captions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to tokenize and un-tokenize sequences.\n",
    "def tokenize(sequences):\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
    "                          oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts(\n",
    "                [[index]])[0])\n",
    "    print(word_seq)\n",
    "\n",
    "# Read files.\n",
    "image_paths, dest_seq = read_training_file(TRAINING_FILE_DIR \\\n",
    "    + 'caption_file.pickle.gz', MAX_LENGTH)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, we cannot afford to keep the entire dataset in memory during training but need to create our training batches on the fly. We handle this task by creating a custom Dataset class that dynamically reads mini-batches of images from disk.\n",
    "\n",
    "In the constructor, we supply the paths to the feature vectors, as well as the tokenized captions. Just as for the language translation example, the recurrent network in the decoder will need the tokenized data both as input and output but shifted by one location and with a START token on the input side. This explains why we provide two variables dest_input_data and dest_target_data to the constructor.\n",
    "\n",
    "The __len__() method is expected to provide the number of examples that our dataset provides, which is simply the number of images.\n",
    "\n",
    "The main functionality in the class is the __getitem__() method, which is expected to return the training data for the example number indicated by the argument idx. The output format of this method depends on what our network requires as input. For a single training example, our network needs a set of feature vectors as input from the encoder side and a shifted version of the target sentence as input to the decoder recurrent network. It also needs the original version of the target sentence as the desired output for the network. Thus, the output from this method should be a three element tuple with two elements representing the two inputs and a single element representing the output. The details become clearer when we later build our training network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to read input files on the fly.\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_paths, dest_input_data,\n",
    "                 dest_target_data):\n",
    "        self.image_paths = image_paths\n",
    "        self.dest_input_data = dest_input_data\n",
    "        self.dest_target_data = dest_target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dest_input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_paths[idx]\n",
    "        dest_input = self.dest_input_data[idx]\n",
    "        dest_target = self.dest_target_data[idx]\n",
    "        image_features = []\n",
    "        file_name = TRAINING_FILE_DIR  \\\n",
    "            + image_id + '.pickle.gzip'\n",
    "        pickle_file = gzip.open(file_name, 'rb')\n",
    "        feature_vector = pickle.load(pickle_file)\n",
    "        pickle_file.close()\n",
    "        return torch.from_numpy(np.array(feature_vector)), \\\n",
    "               torch.from_numpy(np.array(dest_input)), \\\n",
    "               torch.from_numpy(np.array(dest_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor for the ImageCaptionDataset class above assumes that we already have created three arrays with appropriate input data. Two of these arrays (for the recurrent network in the decoder) directly correspond to what we created in the language translation example c14e1_seq2seq_translate. This is shown in the code snippet below, where we also call the constructor for ImageCaptionDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in\n",
    "                        dest_target_token_seq]\n",
    "dest_input_data = pad_sequences(dest_input_token_seq,\n",
    "                                padding='post')\n",
    "dest_target_data = pad_sequences(\n",
    "    dest_target_token_seq, padding='post',\n",
    "    maxlen=len(dest_input_data[0]))\n",
    "\n",
    "# Convert to same precision as model.\n",
    "dest_input_data = dest_input_data.astype(np.int64)\n",
    "dest_target_data = dest_target_data.astype(np.int64)\n",
    "\n",
    "trainset = ImageCaptionDataset(\n",
    "    image_paths, dest_input_data, dest_target_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define the image captioning model. The architecture is shown in Figure 16-4 in Chapter 16. The architecture is a typical encoder-decoder architecture, although most of the encoding has already been done offline. The code snippet below shows the implementation of the model.\n",
    "\n",
    "We start with creating the pretrained VGG19 model. We will not use this for training given that our training data consists of precomputed feature vectors that represent the output of the VGG19 model. However, we do need this model to create feature vectors for other images that we want to use to test the model.\n",
    "\n",
    "Apart from the VGG19 model we implement the actual captioning model. Although it is fairly complex, it should be straightforward to map the functionality to layers in Figure 16-4. We use a similar mechanism for state-handling as we did in c14e1_seq2seq_translate, with the difference that we do not have the encoder as a separate model but we include its layer in the overall model. We can see that in the beginning of the forward method where we compute the h and c state input by running the feature vectors through a couple of layers in the case where use_state is False.\n",
    "\n",
    "There is no pre-existing implementation of the attention mechanism so our forward method manually computes the steps of creating a query and a key, and then does a matrix multiplication between the two and applies softmax to compute the alignment vector used to compute a weighted sum of the inputs that the mechanism attends over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained VGG19 model.\n",
    "vgg19_model = torchvision.models.vgg19(weights='DEFAULT')\n",
    "model_blocks = list(vgg19_model.children())\n",
    "layers = list(model_blocks[0].children())\n",
    "vgg19_model = nn.Sequential(*layers[0:-1])\n",
    "vgg19_model.eval()\n",
    "\n",
    "# Define Captioning model.\n",
    "class CaptioningModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.state = None\n",
    "        self.use_state = False\n",
    "        self.avg_pool2d = nn.AvgPool2d(14)\n",
    "        self.enc_h = nn.Linear(512, LAYER_SIZE)\n",
    "        self.enc_c = nn.Linear(512, LAYER_SIZE)\n",
    "        self.embedding_layer = nn.Embedding(MAX_WORDS, EMBEDDING_WIDTH)\n",
    "        nn.init.uniform_(self.embedding_layer.weight, -0.05, 0.05) # Default is -1, 1.\n",
    "        self.lstm_layer = nn.LSTM(EMBEDDING_WIDTH, LAYER_SIZE, batch_first=True)\n",
    "        self.dec_query_layer = nn.Linear(LAYER_SIZE, 512) # used for attention\n",
    "        self.output_layer = nn.Linear(512+LAYER_SIZE, MAX_WORDS)\n",
    "\n",
    "    def forward(self, feature_vector, caption):\n",
    "        x = self.embedding_layer(caption)\n",
    "        if(self.use_state):\n",
    "            x = self.lstm_layer(x, self.state)\n",
    "        else:\n",
    "            mean_output = self.avg_pool2d(feature_vector)\n",
    "            mean_output = mean_output.view(-1, 512)\n",
    "            h = F.relu(self.enc_h(mean_output))\n",
    "            h = h.view(1, -1, LAYER_SIZE)\n",
    "            c = F.relu(self.enc_c(mean_output))\n",
    "            c = c.view(1, -1, LAYER_SIZE)\n",
    "            x = self.lstm_layer(x, (h, c))\n",
    "        self.state = (x[1][0].detach().clone(), x[1][1].detach().clone()) # Store most recent internal state.\n",
    "\n",
    "        # Attention mechanism begins.\n",
    "        # Compute query for each decoder time step.\n",
    "        # Dimensions = (batch, seq, 512).\n",
    "        query = self.dec_query_layer(x[0])\n",
    "\n",
    "        # Reshape key/data from encoder into 196 spatial locations.\n",
    "        # Dimensions = (batch, features=512, locations=196).\n",
    "        key_data = feature_vector.view(-1, 512, 196)\n",
    "\n",
    "        # Compute normalized attention scores.\n",
    "        # Dimensions = (batch, seq, locations=196).\n",
    "        scores = torch.matmul(query, key_data)\n",
    "        probs = F.softmax(scores, dim = 2)\n",
    "\n",
    "        # Move column in key_data:\n",
    "        # Dimensions = (batch, locations=196, features=512).\n",
    "        key_data = torch.movedim(key_data, 2, 1)\n",
    "\n",
    "        # Last step in attention mechanism.\n",
    "        # Compute weighted sum (attention output).\n",
    "        # Dimensions = (batch, seq, 512).\n",
    "        weighted_sum = torch.matmul(probs, key_data)\n",
    "\n",
    "        # Concatenate with x[0] and feed to output layer.\n",
    "        x = torch.cat((weighted_sum, x[0]), dim=2)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    # Functions to provide explicit control of LSTM state.\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        self.use_state = True\n",
    "        return\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def clear_state(self):\n",
    "        self.use_state = False\n",
    "        return\n",
    "\n",
    "captioning_model = CaptioningModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an optimizer and loss function, transfer the models to the GPU, and create a DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer.\n",
    "captioning_optimizer = torch.optim.RMSprop(captioning_model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Transfer model to GPU.\n",
    "vgg19_model.to(device)\n",
    "captioning_model.to(device)\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to train and evaluate our model, and the code is found below. Just like for the language translation example we build a custom training function. It is similar to the language translation example but with some key differences. We no longer have both an encoder and decoder model but just a single joint model so we only need a single optimizer. To simplify the code we omitted the inner loop that computes loss and accuracy for the test dataset. We are mostly interested in inspecting the generated image captionings anyway. We do this for our four test images (see Figure 16-6) that are neither a part of the training dataset, nor of the test dataset. We run an image through the pretrained VGG19 network to create the corresponding feature vectors. We provide that as input to the image captioning model as well as the START token during the first timestep. We then feed back the output prediction as input to the next timestep in an autoregressive manner and generate the full image caption.\n",
    "\n",
    "For some examples of the type of captions that an equivalent TensorFlow implementation generated and a related discussion, see the book section right around Figure 16-6 in Chapter 16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test repeatedly.\n",
    "for i in range(EPOCHS):\n",
    "    captioning_model.train() # Set model in training mode.\n",
    "    captioning_model.clear_state()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_batches = 0\n",
    "    train_elems = 0\n",
    "    for feature_inputs, dest_inputs, dest_targets in trainloader:\n",
    "        # Move data to GPU.\n",
    "        feature_inputs, dest_inputs, dest_targets = feature_inputs.to(\n",
    "            device), dest_inputs.to(device), dest_targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients.\n",
    "        captioning_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        outputs = captioning_model(feature_inputs, dest_inputs)\n",
    "        loss = loss_function(outputs.view(-1, MAX_WORDS), dest_targets.view(-1))\n",
    "        # Accumulate metrics.\n",
    "        _, indices = torch.max(outputs.data, 2)\n",
    "        train_correct += (indices == dest_targets).sum().item()\n",
    "        train_elems += indices.numel()\n",
    "        train_batches +=  1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update.\n",
    "        loss.backward()\n",
    "        captioning_optimizer.step()\n",
    "    train_loss = train_loss / train_batches\n",
    "    train_acc = train_correct / train_elems\n",
    "\n",
    "    print(f'Epoch {i+1}/{EPOCHS} loss: {train_loss:.4f} - acc: {train_acc:0.4f}')\n",
    "\n",
    "    for filename in TEST_IMAGES:\n",
    "        # Load and preprocess image.\n",
    "        # Resize so shortest side is 256 pixels.\n",
    "        # Crop to center 224x224 region.\n",
    "        image = Image.open(TEST_FILE_DIR + filename).convert('RGB')\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        input_tensor = preprocess(image)\n",
    "\n",
    "        # Rearrange array to have one more\n",
    "        # dimension representing batch size = 1.\n",
    "        inputs = input_tensor.unsqueeze(0)\n",
    "\n",
    "        # Call model.\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature_vector = vgg19_model(inputs)\n",
    "        captioning_model.clear_state()\n",
    "\n",
    "        # Predict sentence word for word.\n",
    "        prev_word_index = START_INDEX\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        for j in range(MAX_LENGTH):\n",
    "            x = np.reshape(np.array(prev_word_index), (1, 1))\n",
    "            # Predict next word and capture internal state.\n",
    "            inputs = torch.from_numpy(x)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = captioning_model(feature_vector, inputs)\n",
    "            preds = outputs.cpu().detach().numpy()[0][0]\n",
    "            state = captioning_model.get_state()\n",
    "            captioning_model.set_state(state)\n",
    "\n",
    "            # Find the most probable word.\n",
    "            prev_word_index = preds.argmax()\n",
    "            pred_seq.append(prev_word_index)\n",
    "            if prev_word_index == STOP_INDEX:\n",
    "                break\n",
    "        tokens_to_words(dest_tokenizer, pred_seq)\n",
    "        print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
