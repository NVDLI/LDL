{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example is very similar to v3_6_digit_classification but the network is modified to use ReLU neruons in the hidden layer, softmax in the output layer, categorical crossentropy as loss function, Adam as optimizer, and a mini-batch size of 64. More context for this code example can be found in video 3.11 \"Programming Example: Improved Digit Classification with PyTorch\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "torch.manual_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Load training dataset into a single batch to compute mean and stddev.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = MNIST(root='./pt_data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=True)\n",
    "data = next(iter(trainloader))\n",
    "mean = data[0].mean()\n",
    "stddev = data[0].std()\n",
    "\n",
    "# Helper function needed to standardize data when loading datasets.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, stddev)])\n",
    "\n",
    "trainset = MNIST(root='./pt_data', train=True, download=True, transform=transform)\n",
    "testset = MNIST(root='./pt_data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we want to use ReLU as activation for the first layer and softmax for the second layer. One thing that is slightly unintuitive is that we omit the activation function altogether for the output layer. The reason is that the cross-entropy loss function implementation in PyTorch is implemented using the inputs to the softmax (also known as logits) instead of the outputs. This makes for an implementation that is more numerically stable.\n",
    "\n",
    "Apart from changing the layer types, we also change the weight initialization scheme to Kaiming (He) for the ReLU layer and Xavier (Glorot) for the softmax layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model. Final activation is omitted since softmax is part of\n",
    "# cross-entropy loss function in PyTorch.\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, 10)\n",
    ")\n",
    "\n",
    "# Retrieve layers for custom weight initialization.\n",
    "layers = next(model.modules())\n",
    "hidden_layer = layers[1]\n",
    "output_layer = layers[3]\n",
    "\n",
    "# Kaiming (He) initialization.\n",
    "nn.init.kaiming_normal_(hidden_layer.weight)\n",
    "nn.init.constant_(hidden_layer.bias, 0.0)\n",
    "\n",
    "# Xavier (Glorot) initialization.\n",
    "nn.init.xavier_uniform_(output_layer.weight)\n",
    "nn.init.constant_(output_layer.bias, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop looks very similar to v3_6_digit_classification. We change to using the Adam optimizer and the CrossEntropyLoss function.\n",
    "\n",
    "Additionally, inside the training loop we no longer convert the targets to one-hot encoded targets. The reason is that the CrossEntropyLoss function object is implemented in a way where it assumes that the input is an integer specifying the index to be hot instead of assuming a one-hot encoded input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Adam optimizer\n",
    "# Cross-entropy loss as loss function.\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Transfer model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Create DataLoader objects that will help create mini-batches.\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Train the model. In PyTorch we have to implement the training loop ourselves.\n",
    "for i in range(EPOCHS):\n",
    "    model.train() # Set model in training mode.\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_batches = 0\n",
    "    for inputs, targets in trainloader:\n",
    "        # Move data to GPU.\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        outputs = model(inputs)\n",
    "        # Cross-entropy loss does not need one-hot targets in PyTorch.\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Accumulate metrics.\n",
    "        _, indices = torch.max(outputs.data, 1)\n",
    "        train_correct += (indices == targets).sum().item()\n",
    "        train_batches +=  1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / train_batches\n",
    "    train_acc = train_correct / (train_batches * BATCH_SIZE)\n",
    "\n",
    "    # Evaluate the model on the test dataset. Identical to loop above but without\n",
    "    # weight adjustment.\n",
    "    model.eval() # Set model in inference mode.\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_batches = 0\n",
    "    for inputs, targets in testloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        _, indices = torch.max(outputs, 1)\n",
    "        test_correct += (indices == targets).sum().item()\n",
    "        test_batches +=  1\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss = test_loss / test_batches\n",
    "    test_acc = test_correct / (test_batches * BATCH_SIZE)\n",
    "\n",
    "    print(f'Epoch {i+1}/{EPOCHS} loss: {train_loss:.4f} - acc: {train_acc:0.4f} - val_loss: {test_loss:.4f} - val_acc: {test_acc:0.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt210_py3100)",
   "language": "python",
   "name": "pt210_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
