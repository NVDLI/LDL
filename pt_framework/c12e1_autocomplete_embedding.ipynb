{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example is similar to c11_e1_autocomplete, but it works on words (encoded with an embedding layer) instead of characters and it does not do beam search. More context for this code example can be found in the section \"Programming Example: Neural Language Model and Resulting Embeddings\" in Chapter 12 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization code below contains a couple of additional imports compared to c11_e1_autocomplete. It might seem odd that we are importing modules from TensorFlow given that this code example is supposed to be implemented in PyTorch. This is not a typo. We simply decided to use some functionality from TensorFlow for preprocessing text. The DL model itself is fully implemented in PyTorch.\n",
    "\n",
    "We also define two new constants MAX_WORDS and EMBEDDING_WIDTH that define the max size of our vocabulary and the dimensionality of the word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Using Keras Tokenizer for simplicity\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "import numpy as np\n",
    "from utilities import train_model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = '../data/frankenstein.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "PREDICT_LENGTH = 3\n",
    "MAX_WORDS = 7500\n",
    "EMBEDDING_WIDTH = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet first reads the input file and splits the text into a list of individual words. The latter is done by using the imported function text_to_word_sequence(), which also removes punctuation and converts the text to lowercase, so we do not need to do that manually in this example. We then create input fragments and associated target words just as in the characterbased example. Because we are working at the granularity of words, these training sentences will be longer from a human perspective, but from the network perspective, they still contain the same number of symbols. However, it will result in fewer training examples than for the character-based example, given that we slide the window forward by a fixed number of words instead of a fixed number of characters for each example. Combined with the fact that the number of unique symbols (the vocabulary) is larger for a word-based system (10,000 words in our case vs. 26 characters), this generally results in a need for a larger text corpus for training a word-based language model than for training a character-based model, but we will still stick with using Frankenstein for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input file.\n",
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8-sig')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Make lower case and split into individual words.\n",
    "text = text_to_word_sequence(text)\n",
    "\n",
    "# Create training examples.\n",
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "    targets.append(text[i + WINDOW_LENGTH])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the training examples into the correct format. Each input word needs to be encoded to a corresponding word index (an integer). This index will then be converted into an embedding by the Embedding layer. The target (output) word should still be one-hot encoded. To simplify how to interpret the output, we want the one-hot encoding to be done in such a way that bit N is hot when the network outputs the word corresponding to index N in the input encoding.\n",
    "\n",
    "We make use of the Tokenizer class. When we construct our tokenizer, we provide an argument num_words = MAX_WORDS that caps the size of the vocabulary. The tokenizer object reserves index 0 to use as a special padding value and index 1 for unknown words. The remaining 9,998 indices (MAX_WORDS was set to 10,000) are used to represent words in the vocabulary.\n",
    "\n",
    "The padding value (index 0) can be used to make all training examples within the same batch have the same length. The Embedding layer can be instructed to ignore this value, so the network does not train on the padding values.\n",
    "\n",
    "Index 1 is reserved for UNKnown (UNK) words because we have declared UNK as an out-of-vocabulary (oov) token. When using the tokenizer to convert text to tokens, any word that is not in the vocabulary will be replaced by the word UNK. Similarly, if we try to convert an index that is not assigned to a word, the tokenizer will return UNK. If we do not set the oov_token parameter, it will simply ignore such words/indices.\n",
    "\n",
    "After instantiating our tokenizer, we call fit_on_texts() with our entire text corpus, which will result in the tokenizer assigning indices to words. We can then use the function texts_to_sequences to convert a text string into a list of indices, where unknown words will be assigned the index 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to indices.\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(text)\n",
    "fragments_indexed = tokenizer.texts_to_sequences(fragments)\n",
    "targets_indexed = tokenizer.texts_to_sequences(targets)\n",
    "\n",
    "# Convert to appropriate input and output formats.\n",
    "X = np.array(fragments_indexed, dtype=np.int64)\n",
    "y = np.zeros(len(targets_indexed), dtype=np.int64)\n",
    "for i, target_index in enumerate(targets_indexed):\n",
    "    y[i] = target_index[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous example, we split the data into a training set and a test set and create Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set.\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=0)\n",
    "\n",
    "# Create Dataset objects.\n",
    "trainset = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "testset = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create the model, but we do this differently than we have done in previous examples, both to illustrate some new constructs but also to provide more flexibility in what we are building. Instead of instantiating a Sequential model with standard layers, we build a fully customized model. This is done by extending the nn.Module class. We already saw examples of that when we built custom layers, and we use a similar methodolgoy when building a fully custom model.\n",
    "\n",
    "To understand the details of this model we first need to take a detour and discuss some details of how we plan to use the model to do predictions. We do this a little bit differently than in the previous chapters. Instead of feeding a string of symbols as input to the model, we feed it only a single symbol at a time. This is an alternative implementation compared to the implementation in Chapter 11, where we repeatedly fed the model a growing sequence of characters. To clarify, in c11e1_autocomplete, we first fed the model the sequence 'the body ', which resulted in the character ‘w’ as output. In the next step, we fed it 'the body w', followed by 'the body wh', and so on. That is, for every prediction, we started over from the beginning. Had we instead used the implementation from this chapter, we would have fed it 't', 'h', 'e', ' ', 'b', 'o', 'd', 'y', ' ', which would have resulted in an output 'w', and we would then just feed that character back as input.\n",
    "\n",
    "The scheme used in this chapter has a subtle implication, which has to do with dependencies between multiple consecutive calls to the model. In Chapter 11, we did not have an expectation that the inputs to the first prediction should impact the second prediction. We probably would have found it odd if they had because that would mean that the output value we would get from the model could be different for two consecutive calls that had identical input values. This has been implicitly taken care of in the previous models by the internal state (c and h for LSTM cells) being reset to 0 between each call to the model.\n",
    "\n",
    "In this chapter, we do not want this behavior. We want the LSTM layers to retain their c and h states from one call to another so that the outputs of subsequent calls to the model will depend on the prior calls to the model. We provide this functionality with our own custom methods set_state(), get_state(), and clear_state().\n",
    "\n",
    "We are now ready to describe the details of our model. The __init__() function will be called a single time when the model is created. When buildging a custom model, this is the place to put any initialization code, such as declaring the layers to be used by the model as well as any other variables that will be needed.\n",
    "\n",
    "We start with declaring two variables state and use_state to control how we handle initialization of the internal state of the LSTM layers (see below).\n",
    "\n",
    "For the model, we start with declaring an Embedding layer with MAX_WORDS as input size and EMBEDDING_WIDTH as output size. We adjust the weight initialization to use uniform random numbers between -0.05 and 0.05, as opposed to the default range of -1.0 to 1.0. We did this to match the range used in our TensorFlow examples. \n",
    "\n",
    "Next we declare an LSTM module containing two LSTM layers, similar to c11e1_autocomplete. We also add a Dropout layer, followed by two fully connected layers, with ReLU activation in-between. You might wonder why we no longer need the custom layer between the LSTM module and the Dropout layer, when we needed that in c11e1_autocomplete. The reason is that in a custom model the layers are not automatically connected to each other but we handle that explicitly in the forward() method. In fact, we could have declared the layers in any order in the __init__ function, and the model would still do the same thing.\n",
    "\n",
    "The behavior of the model is defined in the forward method. We feed the inputs to the embedding layer and the resulting outputs are temporarily stored in the variable x. Next we arrive at the custom state handling code for the LSTM layers. If our variable use_state is False (the value we initialized it to), we will call the LSTM layers with just x as input. This implies that the LSTM module will use 0 as initial h and c states. However, if use_state is set to true, we will also supply the variable self.state as input to the LSTM layers. In that case the LSTM layers will use these states as their initial states instead of 0.\n",
    "\n",
    "After the LSTM layers has been called, we retrieve the resulting internal state and store it in self.state so it can be used as input for the next timestep. Apart from calling detach() we also call clone(), which makes a copy of the state so it does not change under the hood by the layers themselves if they are later called with new inputs.\n",
    "\n",
    "We then feed the output from the top LSTM layer through the Dropout layer. The indices after variable x results in selecting the final timestep for the top LSTM layer. That is, this is equivalent to the functionality of the custom layer in c11e1_autocomplete. Given that we are building a custom model and have explicit control of how the layers are connected, there is no need to declare a custom layer.\n",
    "\n",
    "The rest of the forward method is straight forward and we feed the output from the Dropout layer through a Linear layer, followed by a ReLU layer, and finally another Linear layer that represents the Softmax layer but without its activation function, which is later included in the loss function.\n",
    "\n",
    "Now let us detail the intended use of set_state, get_state, and clear_state. At the beginning of time the use_state variable is set to False. When calling the model, it will use 0 as internal state. After each call to the model, we can retrieve the resulting state by calling get_state. If we later want the model to use this state as its initial state, we simply call set_state and supply that state to the model, before we call the model with its input data. If we later decide that we want to start with a cleared internal state again, we simply call the clear_state method. This concludes the description of the custom model.\n",
    "\n",
    "Next we instantiate the model, select our normal optimizer and loss function, and finally train the model. Our training process does not make use of the just described state handling and the model will simply use 0 as initial state for each call to the model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model.\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.state = None\n",
    "        self.use_state = False\n",
    "        self.embedding_layer = nn.Embedding(MAX_WORDS, EMBEDDING_WIDTH)\n",
    "        nn.init.uniform_(self.embedding_layer.weight, -0.05, 0.05) # Default is -1, 1.\n",
    "        self.lstm_layers = nn.LSTM(EMBEDDING_WIDTH, 128, num_layers=2, dropout=0.2, batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(0.2)\n",
    "        self.linear_layer = nn.Linear(128, 128)\n",
    "        self.relu_layer = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(128, MAX_WORDS)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        if(self.use_state):\n",
    "            x = self.lstm_layers(x, self.state)\n",
    "        else:\n",
    "            x = self.lstm_layers(x)\n",
    "        self.state = (x[1][0].detach().clone(), x[1][1].detach().clone()) # Store most recent internal state.\n",
    "        x = self.dropout_layer(x[1][0][1])\n",
    "        x = self.linear_layer(x)\n",
    "        x = self.relu_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    # Functions to provide explicit control of LSTM state.\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        self.use_state = True\n",
    "        return\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def clear_state(self):\n",
    "        self.use_state = False\n",
    "        return\n",
    "\n",
    "model = LanguageModel()\n",
    "\n",
    "# Loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model.\n",
    "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset,\n",
    "            optimizer, loss_function, 'acc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we are ready to use it to do predictions. The first loop in the code snippet below feeds an initial sequence of words to the model. Note how we call get_state after each call followed by set_state, which instructs the model to use this stat instead of reseting it before each word.\n",
    "\n",
    "The second loop contains the autoregression logic where we identify the word that the model predicts as highest probability and then feed that as input to the model in the next timestep. To simplify the implementation, we do not do beam search this time around but simply predict the most probable word at each timestep.\n",
    "\n",
    "We conclude with printing the resulting autocompleted text sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide beginning of sentence and\n",
    "# predict next words in a greedy manner.\n",
    "first_words = ['i', 'saw']\n",
    "first_words_indexed = tokenizer.texts_to_sequences(\n",
    "    first_words)\n",
    "model.clear_state()\n",
    "predicted_string = ''\n",
    "# Feed initial words to the model.\n",
    "for i, word_index in enumerate(first_words_indexed):\n",
    "    x = np.zeros((1, 1), dtype=np.int64)\n",
    "    x[0][0] = word_index[0]\n",
    "    predicted_string += first_words[i]\n",
    "    predicted_string += ' '\n",
    "    inputs = torch.from_numpy(x)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    y_predict = outputs.cpu().detach().numpy()[0]\n",
    "    state = model.get_state()\n",
    "    model.set_state(state)\n",
    "# Predict PREDICT_LENGTH words.\n",
    "for i in range(PREDICT_LENGTH):\n",
    "    new_word_index = np.argmax(y_predict)\n",
    "    word = tokenizer.sequences_to_texts(\n",
    "        [[new_word_index]])\n",
    "    x[0][0] = new_word_index\n",
    "    predicted_string += word[0]\n",
    "    predicted_string += ' '\n",
    "    inputs = torch.from_numpy(x)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    y_predict = outputs.cpu().detach().numpy()[0]\n",
    "    state = model.get_state()\n",
    "    model.set_state(state)\n",
    "print(predicted_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the preceding code had to do with building and using a language model. The next code snippet adds some functionality to explore the learned embeddings. We first read out the word embeddings from the Embedding layer by accessing the weight variable from the model's first layer, which represents the Embedding layer. We move the weights back to the CPU and convert them to NumPy format. We first have to call detach() and clone() (see section \"Explicit moves of data between NumPy and PyTorch\n",
    "\" in Appendix I for a brief discussion about these constructs).\n",
    "\n",
    "We then declare a list of a number of arbitrary lookup words. This is followed by a loop that does one iteration per lookup word. The loop uses the Tokenizer to convert the lookup word to a word index, which is then used to retrieve the corresponding word embedding. The Tokenizer functions are generally assumed to work on lists. Therefore, although we work with a single word at a time, we need to provide it as a list of size 1, and then we need to retrieve element zero ([0]) from the output.\n",
    "\n",
    "Once we have retrieved the corresponding word embedding, we loop through all the other embeddings and calculate the Euclidean distance to the embedding for the lookup word using the NumPy function norm(). We add the distance and the corresponding word to the dictionary word_indices. Once we have calculated the distance to each word, we simply sort the distances and retrieve the five word indices that correspond to the word embeddings that are closest in vector space. We use the Tokenizer to convert these indices back to words and print them and their corresponding distances.\n",
    "\n",
    "See table 12-4 in Chapter 12 for examples of how words were grouped when we did this experiment with an equivalent TensorFlow implementation. Note that given the stochastic nature of this process, your model will likely produce a quite different output, but there should be a high probability that your model produces a correct sentence. Note that because we replaced rare words with UNK (for UNKnown) in the training set, the model may well produce an output sentence that includes UNK as a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore embedding similarities.\n",
    "it = model.modules()\n",
    "next(it)\n",
    "embeddings = next(it).weight\n",
    "embeddings = embeddings.detach().clone().cpu().numpy()\n",
    "\n",
    "lookup_words = ['the', 'saw', 'see', 'of', 'and',\n",
    "                'monster', 'frankenstein', 'read', 'eat']\n",
    "for lookup_word in lookup_words:\n",
    "    lookup_word_indexed = tokenizer.texts_to_sequences(\n",
    "        [lookup_word])\n",
    "    print('words close to:', lookup_word)\n",
    "    lookup_embedding = embeddings[lookup_word_indexed[0]]\n",
    "    word_indices = {}\n",
    "    # Calculate distances.\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        distance = np.linalg.norm(\n",
    "            embedding - lookup_embedding)\n",
    "        word_indices[distance] = i\n",
    "    # Print sorted by distance.\n",
    "    for distance in sorted(word_indices.keys())[:5]:\n",
    "        word_index = word_indices[distance]\n",
    "        word = tokenizer.sequences_to_texts([[word_index]])[0]\n",
    "        print(word + ': ', distance)\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
