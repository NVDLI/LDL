{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to build a neural machine translation network. It is a sequence-to-sequence network based on an encoder-decoder architecture. More context for this code example can be found in video 7.3 \"Programming Example: Neural Machine Translation with PyTorch\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n",
    "\n",
    "The data used to train the model is expected to be in the file ../data/fra.txt.\n",
    "We begin by importing modules that we need for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence \\\n",
    "    import pad_sequences\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some constants. We specify a vocabulary size of 10,000 symbols, out of which four indices are reserved for padding, out-of-vocabulary words (denoted as UNK), START tokens, and STOP tokens. Our training corpus is large, so we set the parameter READ_LINES to the number of lines in the input file we want to use in our example (60,000). Our layers consist of 256 units (LAYER_SIZE), and the embedding layers output 128 dimensions (EMBEDDING_WIDTH). We use 20% (TEST_PERCENT) of the dataset as test set and further select 20 sentences (SAMPLE_SIZE) to inspect in detail during training. We limit the length of the source and destination sentences to, at most, 60 words (MAX_LENGTH). Finally, we provide the path to the data file, where each line is expected to contain two versions of the same sentence (one in each language) separated by a tab character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_LINES = 60000\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128\n",
    "TEST_PERCENT = 0.2\n",
    "SAMPLE_SIZE = 20\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "SRC_DEST_FILE_NAME = '../data/fra.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the function used to read the input data file and do some initial processing. Each line is split into two strings, where the first contains the sentence in the destination language and the second contains the sentence in the source language. We use the function text_to_word_sequence() to clean the data somewhat (make everything lowercase and remove punctuation) and split each sentence into a list of individual words. If the list (sentence) is longer than the maximum allowed length, then it is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read file.\n",
    "def read_file_combined(file_name, max_len):\n",
    "    file = open(file_name, 'r', encoding='utf-8')\n",
    "    src_word_sequences = []\n",
    "    dest_word_sequences = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i == READ_LINES:\n",
    "            break\n",
    "        pair = line.split('\\t')\n",
    "        word_sequence = text_to_word_sequence(pair[1])\n",
    "        src_word_sequence = word_sequence[0:max_len]\n",
    "        src_word_sequences.append(src_word_sequence)\n",
    "        word_sequence = text_to_word_sequence(pair[0])\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    file.close()\n",
    "    return src_word_sequences, dest_word_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows functions used to turn sequences of words into\n",
    "sequences of tokens, and vice versa. We call tokenize() a single time for each\n",
    "language, so the argument sequences is a list of lists where each of the inner\n",
    "lists represents a sentence. The Tokenizer class assigns indices to the most\n",
    "common words and returns either these indices or the reserved OOV_INDEX\n",
    "for less common words that did not make it into the vocabulary. We tell the\n",
    "Tokenizer to use a vocabulary of 9998 (MAX_WORDS-2)—that is, use only\n",
    "indices 0 to 9997, so that we can use indices 9998 and 9999 as our START and\n",
    "STOP tokens (the Tokenizer does not support the notion of START and STOP\n",
    "tokens but does reserve index 0 to use as a padding token and index 1 for outof-\n",
    "vocabulary words). Our tokenize() function returns both the tokenized\n",
    "sequence and the Tokenizer object itself. This object will be needed anytime we\n",
    "want to convert tokens back into words.\n",
    "\n",
    "The function tokens_to_words() requires a Tokenizer and a list of indices. We simply check for the reserved indices: If we find a match, we replace them with hardcoded strings, and if we find no match, we let the Tokenizer convert the index to the corresponding word string. The Tokenizer expects a list of lists of indices and returns a list of strings, which is why we need to call it with [[index]] and then select the 0th element to arrive at a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to tokenize and un-tokenize sequences.\n",
    "def tokenize(sequences):\n",
    "    # \"MAX_WORDS-2\" used to reserve two indices\n",
    "    # for START and STOP.\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
    "                          oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts(\n",
    "                [[index]])[0])\n",
    "    print(word_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these helper functions, it is trivial to read the input data\n",
    "file and convert into tokenized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and tokenize.\n",
    "src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME,\n",
    "                                       MAX_LENGTH)\n",
    "src_tokenizer, src_token_seq = tokenize(src_seq)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to arrange the data into arrays that can be used for training and testing. The following example provides some insight into what we need as input and output for a single training example, where src_input is the input to the encoder network, dest_input is the input to the decoder network, and dest_target is the desired output from the decoder network:\n",
    "\n",
    "src_input = [PAD, PAD, PAD, id(\"je\"), id(\"suis\"), id(\"étudiant\")]\n",
    "dest_input = [START, id(\"i\"), id(\"am\"), id(\"a\"), id(\"student\"), STOP, PAD, PAD]\n",
    "dest_target = [one_hot_id(\"i\"), one_hot_id(\"am\"), one_hot_id(\"a\"), one_hot_id(\"student\"), one_hot_id(STOP), one_hot_id(PAD), one_hot_id(PAD), one_hot_id(PAD)]\n",
    "\n",
    "In the example, id(string) refers to the tokenized index of the string, and one_hot_id is the one-hot encoded version of the index. We have assumed that the longest source sentence is six words, so we padded src_input to be of that length. Similarly, we have assumed that the longest destination sentence is eight words including START and STOP tokens, so we padded both dest_input and dest_target to be of that length. Note how the symbols in dest_input are offset by one location compared to the symbols in dest_target because when we later do inference, the inputs into the decoder network will be coming from the output of the network for the previous timestep. Although this example has shown the training example as being lists, in reality, they will be rows in NumPy arrays, where each array contains multiple training examples.\n",
    "\n",
    "The padding is done to ensure that we can use mini-batches for training. That is, all source sentences need to be the same length, and all destination sentences need to be the same length. We pad the source input at the beginning (known as prepadding) and the destination at the end (known as postpadding).\n",
    "\n",
    "The code snippet below shows a compact way of creating the three arrays that we need. The first two lines create two new lists, each containing the destination sequences but the first (dest_target_token_seq) also augmented with STOP_INDEX after each sequence and the second (dest_input_token_seq) augmented with both START_INDEX and STOP_INDEX. It is easy to miss that dest_input_token_seq has a STOP_INDEX, but that falls out naturally because it is created from the dest_target_token_seq for which a STOP_INDEX was just added to each sentence.\n",
    "\n",
    "Next, we call pad_sequences() on both the original src_input_data list (of lists) and on these two new destination lists. The pad_sequences() function pads the sequences with the PAD value and then returns a NumPy array. The default behavior of pad_sequences is to do prepadding, and we do that for the source sequence but explicitly ask for postpadding for the destination sequences.\n",
    "\n",
    "We conclude with converting the data type to np.int64 to match what PyTorch later requires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in\n",
    "                        dest_target_token_seq]\n",
    "src_input_data = pad_sequences(src_token_seq)\n",
    "dest_input_data = pad_sequences(dest_input_token_seq,\n",
    "                                padding='post')\n",
    "dest_target_data = pad_sequences(\n",
    "    dest_target_token_seq, padding='post', maxlen\n",
    "    = len(dest_input_data[0]))\n",
    "\n",
    "# Convert to same precision as model.\n",
    "src_input_data = src_input_data.astype(np.int64)\n",
    "dest_input_data = dest_input_data.astype(np.int64)\n",
    "dest_target_data = dest_target_data.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet demonstrates how we can manually split our dataset into a training dataset and a test dataset. We split the dataset by first creating a list test_indices, which contains a 20% (TEST_PERCENT) subset of all the numbers from 0 to N−1, where N is the size of our original dataset. We then create a list train_indices, which contains the remaining 80%. We can now use these lists to select a number of rows in the arrays representing the dataset and create two new collections of arrays, one to be used as training set and one to be used as test set. Finally, we create a third collection of arrays, which only contains 20 (SAMPLE_SIZE) random examples from the test dataset. We will use them to inspect the resulting translations in detail, but since that is a manual process, we limit ourselves to a small number of sentences.\n",
    "\n",
    "Finally, we convert the NumPy arrays to PyTorch tensors and create Dataset objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set.\n",
    "rows = len(src_input_data[:,0])\n",
    "all_indices = list(range(rows))\n",
    "test_rows = int(rows * TEST_PERCENT)\n",
    "test_indices = random.sample(all_indices, test_rows)\n",
    "train_indices = [x for x in all_indices if x not in test_indices]\n",
    "\n",
    "train_src_input_data = src_input_data[train_indices]\n",
    "train_dest_input_data = dest_input_data[train_indices]\n",
    "train_dest_target_data = dest_target_data[train_indices]\n",
    "\n",
    "test_src_input_data = src_input_data[test_indices]\n",
    "test_dest_input_data = dest_input_data[test_indices]\n",
    "test_dest_target_data = dest_target_data[test_indices]\n",
    "\n",
    "# Create a sample of the test set that we will inspect in detail.\n",
    "test_indices = list(range(test_rows))\n",
    "sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n",
    "sample_input_data = test_src_input_data[sample_indices]\n",
    "sample_target_data = test_dest_target_data[sample_indices]\n",
    "\n",
    "# Create Dataset objects.\n",
    "trainset = TensorDataset(torch.from_numpy(train_src_input_data),\n",
    "                         torch.from_numpy(train_dest_input_data),\n",
    "                         torch.from_numpy(train_dest_target_data))\n",
    "testset = TensorDataset(torch.from_numpy(test_src_input_data),\n",
    "                         torch.from_numpy(test_dest_input_data),\n",
    "                         torch.from_numpy(test_dest_target_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build our model. The encoder consists of an embedding layer and two LSTM layers. The decoder consists of an embedding layer, two LSTM layers, and a fully connected softmax layer. We define these as two separate models, but we will use them together as an encoder-decoder model.\n",
    "\n",
    "The code snippet below contains the implementation of the encoder model. One thing to note is that it will output the full output from the LSTM object. That is, it will output a tuple containing both output state and internal state for all layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models.\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(MAX_WORDS, EMBEDDING_WIDTH)\n",
    "        nn.init.uniform_(self.embedding_layer.weight, -0.05, 0.05) # Default is -1, 1.\n",
    "        self.lstm_layers = nn.LSTM(EMBEDDING_WIDTH, LAYER_SIZE, num_layers=2, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.lstm_layers(x)\n",
    "        return x[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the implementation of the decoder model. In addition to the sentence in the destination language, it needs the output state from the encoder model. We manage the input state explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.state = None\n",
    "        self.use_state = False\n",
    "        self.embedding_layer = nn.Embedding(MAX_WORDS, EMBEDDING_WIDTH)\n",
    "        nn.init.uniform_(self.embedding_layer.weight, -0.05, 0.05) # Default is -1, 1.\n",
    "        self.lstm_layers = nn.LSTM(EMBEDDING_WIDTH, LAYER_SIZE, num_layers=2, batch_first=True)\n",
    "        self.output_layer = nn.Linear(LAYER_SIZE, MAX_WORDS)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        if(self.use_state):\n",
    "            x = self.lstm_layers(x, self.state)\n",
    "        else:\n",
    "            x = self.lstm_layers(x)\n",
    "        self.state = (x[1][0].detach().clone(), x[1][1].detach().clone()) # Store most recent internal state.\n",
    "        x = self.output_layer(x[0])\n",
    "        return x\n",
    "\n",
    "    # Functions to provide explicit control of LSTM state.\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "        self.use_state = True\n",
    "        return\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def clear_state(self):\n",
    "        self.use_state = False\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet instantitates the two models, and creates two optimizers, one for each model. We decided to use RMSProp as optimizer because some experiments indicate that it performs better than Adam for this specific model. We use CrossEntropyLoss as usual.\n",
    "\n",
    "We transfer the models to the GPU and create a DataLoader object for both the training and test dataset. We have not had to do this lately because it has been included in our train_model funtion that was reused for all recent examples. We cannot use that function in this example because it does not support the more complex encoder-decoder model that we want to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = EncoderModel()\n",
    "decoder_model = DecoderModel()\n",
    "\n",
    "# Loss functions and optimizer.\n",
    "encoder_optimizer = torch.optim.RMSprop(encoder_model.parameters(), lr=0.001)\n",
    "decoder_optimizer = torch.optim.RMSprop(decoder_model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Using a custom training loop instead of our standard training function.\n",
    "# Transfer model to GPU.\n",
    "encoder_model.to(device)\n",
    "decoder_model.to(device)\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final code snippet shows hos to train and test the model. It is a modified version of the training loop that we had implemented in the train_model function (see utilities.py for comparison). We focus on describing the differences.\n",
    "\n",
    "We need to set both the encoder and decoder model in training mode by calling the train() method. The same applies for the calls to zero_grad() and step() on the optimizers. Additionally, our DataLoader objects will now return two sets of inputs (src_inputs and dest_inputs) to supply input values both to the encoder and decoder models.\n",
    "\n",
    "For the forward pass we first invoke the encoder model. We then read out its state and use that to set the state for the decoder model, followed by invoking the decoder model as well.\n",
    "\n",
    "We have also modified how the accuracy metric is calculated. Accuracy isn't necessarily the best metric for machine translation and one can also envision different ways of computing accuracy. The way we compute it in this example is that we determine how many of the words match the expected targets, instead of looking at if a sentence fully matches the target sentence.\n",
    "\n",
    "The second inner loop below evaluates the model on the test dataset. Just as in our usual train_model function, this loop largely mimics the first inner loop but without running a backward pass and adjusting weights.\n",
    "\n",
    "We then move on to a third inner loop below that was not present in our train_model function. This loop uses the model to produce and print out translations for our 20 sample examples. We do this to gain some insight into what the translations look like. We first invoke the encoder model with the sentence to translate and retrieve the model internal state. We then use this internal state as a starting point for the decoder to generate a translation, using the START token as input for the first time step. The loop that generates the translation is very similar to the code for the language model. After all, the decoder is no different than a language model that generates a sentence based on an encoder-generated initial state representing the sentence to translate. The loop iterates until the model produces a STOP token or until a given number of words have been produced. Finally, we convert the produced tokenized sequences into the corresponding word sequences and print them out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test repeatedly.\n",
    "for i in range(EPOCHS):\n",
    "    encoder_model.train() # Set model in training mode.\n",
    "    decoder_model.train() # Set model in training mode.\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_batches = 0\n",
    "    train_elems = 0\n",
    "    for src_inputs, dest_inputs, dest_targets in trainloader:\n",
    "        # Move data to GPU.\n",
    "        src_inputs, dest_inputs, dest_targets = src_inputs.to(\n",
    "            device), dest_inputs.to(device), dest_targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients.\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        encoder_state = encoder_model(src_inputs)\n",
    "        decoder_model.set_state(encoder_state)\n",
    "        outputs = decoder_model(dest_inputs)\n",
    "        loss = loss_function(outputs.view(-1, MAX_WORDS), dest_targets.view(-1))\n",
    "        # Accumulate metrics.\n",
    "        _, indices = torch.max(outputs.data, 2)\n",
    "        train_correct += (indices == dest_targets).sum().item()\n",
    "        train_elems += indices.numel()\n",
    "        train_batches +=  1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update.\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / train_batches\n",
    "    train_acc = train_correct / train_elems\n",
    "\n",
    "    # Evaluate the model on the test dataset.\n",
    "    encoder_model.eval() # Set model in inference mode.\n",
    "    decoder_model.eval() # Set model in inference mode.\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_batches = 0\n",
    "    test_elems = 0\n",
    "    for src_inputs, dest_inputs, dest_targets in testloader:\n",
    "        # Move data to GPU.\n",
    "        src_inputs, dest_inputs, dest_targets = src_inputs.to(\n",
    "            device), dest_inputs.to(device), dest_targets.to(device)\n",
    "        encoder_state = encoder_model(src_inputs)\n",
    "        decoder_model.set_state(encoder_state)\n",
    "        outputs = decoder_model(dest_inputs)\n",
    "        loss = loss_function(outputs.view(-1, MAX_WORDS), dest_targets.view(-1))\n",
    "        _, indices = torch.max(outputs, 2)\n",
    "        test_correct += (indices == dest_targets).sum().item()\n",
    "        test_elems += indices.numel()\n",
    "        test_batches +=  1\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss = test_loss / test_batches\n",
    "    test_acc = test_correct / test_elems\n",
    "    print(f'Epoch {i+1}/{EPOCHS} loss: {train_loss:.4f} - acc: {train_acc:0.4f} - val_loss: {test_loss:.4f} - val_acc: {test_acc:0.4f}')\n",
    "\n",
    "    # Loop through samples to see result\n",
    "    for (test_input, test_target) in zip(sample_input_data,\n",
    "                                         sample_target_data):\n",
    "        # Run a single sentence through encoder model.\n",
    "        x = np.reshape(test_input, (1, -1))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        inputs = inputs.to(device)\n",
    "        last_states = encoder_model(inputs)\n",
    "\n",
    "        # Provide resulting state and START_INDEX as input\n",
    "        # to decoder model.\n",
    "        decoder_model.set_state(last_states)\n",
    "        prev_word_index = START_INDEX\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        for j in range(MAX_LENGTH):\n",
    "            x = np.reshape(np.array(prev_word_index), (1, 1))\n",
    "            # Predict next word and capture internal state.\n",
    "            inputs = torch.from_numpy(x)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = decoder_model(inputs)\n",
    "            preds = outputs.cpu().detach().numpy()[0][0]\n",
    "            state = decoder_model.get_state()\n",
    "            decoder_model.set_state(state)\n",
    "\n",
    "            # Find the most probable word.\n",
    "            prev_word_index = preds.argmax()\n",
    "            pred_seq.append(prev_word_index)\n",
    "            if prev_word_index == STOP_INDEX:\n",
    "                break\n",
    "        tokens_to_words(src_tokenizer, test_input)\n",
    "        tokens_to_words(dest_tokenizer, test_target)\n",
    "        tokens_to_words(dest_tokenizer, pred_seq)\n",
    "        print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt210_py3100)",
   "language": "python",
   "name": "pt210_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
