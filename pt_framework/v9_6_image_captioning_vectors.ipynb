{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example generates feature vectors corresponding to images in the input dataset. These feature vectors are used by the image captioning network in the code example v9_6_image_captioning. More context for this code example can be found in video 9.6 \"Programming Example: Image Captioning with PyTorch\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614). This is notebook 1 of 2 for this example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programming example assumes that the following resources from the COCO dataset are available:\n",
    "The file captions_train2014.json should be located in the directory ../data/coco/\n",
    "All the training images should be located in the directory ../data/coco/train2014/\n",
    "\n",
    "The resulting feature vectors will be stored in the directory pt_data/feature_vectors/\n",
    "\n",
    "The import statements are shown in the first code snippet below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAINING_FILE_DIR = '../data/coco/'\n",
    "OUTPUT_FILE_DIR = 'pt_data/feature_vectors/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parts of the dataset that we will use are contained in two resources. The first resource is a json file that contains captions as well as filenames and some other information for the images. We make the assumption that you have placed that file in the directory pointed to by the variable TRAINING_FILE_DIR. The images themselves are stored as individual image files and are assumed to be located in a directory named train2014 in the directory pointed to by TRAINING_FILE_DIR. The COCO dataset contains elaborate tools to parse and read the rich information about the various images, but because we are only interested in the image captions, we choose to directly access the json file and extract the limited data that we need ourselves. The code snippet below opens the json file and creates a dictionary that, for each image, maps a unique key to a list of strings. The first string in each list represents the image filename, and the subsequent strings are alternative captions for the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_FILE_DIR \\\n",
    "          + 'captions_train2014.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "image_dict = {}\n",
    "for image in data['images']:\n",
    "    image_dict[image['id']] = [image['file_name']]\n",
    "for anno in data['annotations']:\n",
    "    image_dict[anno['image_id']].append(anno['caption'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create our pretrained VGG19 model, which is done in the next code snippet. We first obtain the full pretrained VGG19 model. We then create a new model but drop the fully connected layers at the top of the model. Looking at the code, it is non-obvious how we drop multiple layers. It turns out that the layers are grouped into three blocks of layers. The first block contains convolutional layers and pooling layers. The second and third blocks contain the fully connected layers. That is, by selecting only block 0, we drop a number of layers. We then drop the last layer in block 0, which is a max-pooling layer. That is, the output from our new model is the top-most convolutional layer from the original model.\n",
    "\n",
    "We then transfer this new model to the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network without top layers.\n",
    "model = torchvision.models.vgg19(weights='DEFAULT')\n",
    "model_blocks = list(model.children())\n",
    "layers = list(model_blocks[0].children())\n",
    "model = nn.Sequential(*layers[0:-1])\n",
    "model.eval()\n",
    "\n",
    "# Transfer model to GPU\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run all the images through the network and extract the feature vectors and save to disk. This is done by the code snippet below. We traverse the dictionary to obtain the image file names. Every loop iteration does the processing for a single image and saves the feature vectors for that one image in a single file. Before running the image through the network, we perform some preprocessing. The image sizes in the COCO dataset vary from image to image, so we first resize it so the shortest side is 256 pixels, and then we crop the center 224Ã—224 region of the resulting image. We also normalize the pixel values using mean and standard deviation documented at pytortch.org.\n",
    "\n",
    "Next we run the image through the network. The output of the network will be a tensor with the shape (1, 14, 14, 512) representing the results from a batch of images where the first dimension indicates that the batch size is 1. Therefore, we extract the first (and only) element from this tensor and convert it to a NumPy array that we save as a gzipped pickle file with the same name as the image but with the extension .pickle.gz in the directory feature_vectors. When we have looped through all images, we also save the dictionary file as caption_file. pickle.gz so we do not need to parse the json file again later in the code that does the actual training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all images through the network and save the output.\n",
    "for i, key in enumerate(image_dict.keys()):\n",
    "    if i % 1000 == 0:\n",
    "        print('Progress: ' + str(i) + ' images processed')\n",
    "    item = image_dict.get(key)\n",
    "    filename = TRAINING_FILE_DIR + 'train2014/' + item[0]\n",
    "\n",
    "    # Load and preprocess image.\n",
    "    # Resize so shortest side is 256 pixels.\n",
    "    # Crop to center 224x224 region.\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_tensor = preprocess(image)\n",
    "\n",
    "    # Rearrange array to have one more\n",
    "    # dimension representing batch size = 1.\n",
    "    inputs = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Call model and save resulting tensor to disk.\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        y = model(inputs)[0].cpu().numpy()\n",
    "    save_filename = OUTPUT_FILE_DIR + \\\n",
    "        item[0] + '.pickle.gzip'\n",
    "    pickle_file = gzip.open(save_filename, 'wb')\n",
    "    pickle.dump(y, pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "# Save the dictionary containing captions and filenames.\n",
    "save_filename = OUTPUT_FILE_DIR + 'caption_file.pickle.gz'\n",
    "pickle_file = gzip.open(save_filename, 'wb')\n",
    "pickle.dump(image_dict, pickle_file)\n",
    "pickle_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt210_py3100)",
   "language": "python",
   "name": "pt210_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
