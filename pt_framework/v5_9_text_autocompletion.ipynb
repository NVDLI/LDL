{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to use an LSTM-based neural network and beam search to do text autocompletion. More context for this code example can be found in video 5.9 \"Programming Example: Text Autocompletion with PyTorch\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization code is shown in the first code snippet. Apart from the import statements, we need to provide the path to the text file to use for training. We also define two variables, WINDOW_LENGTH and WINDOW_STEP, which are used to control the process of splitting up this text file into multiple training examples. The other three variables control the beam-search algorithm and are described shortly. The text used to train the model is assumed to be in the file ../data/frankenstein.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utilities import train_model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = '../data/frankenstein.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "BEAM_SIZE = 8\n",
    "NUM_LETTERS = 11\n",
    "MAX_LENGTH = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet opens and reads the content of the file, converts it all into lowercase, and replaces double spaces with single spaces. To enable us to easily one-hot encode each character, we want to assign a monotonically increasing index to each character. This is done by first creating a list of unique characters. Once we have that list, we can loop over it and assign an incrementing index to each character. We do this twice to create one dictionary (a hash table) that maps from character to index and a reverse dictionary from index to character. These will come in handy later when we want to convert text into one-hot encoded input to the network as well as when we want to convert one-hot encoded output into characters. Finally, we initialize a variable encoding_width with the count of unique characters, which will be the width of each one-hot encoded vector that represents a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input file.\n",
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8-sig')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Make lower-case and remove newline and extra spaces.\n",
    "text = text.lower()\n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('  ', ' ')\n",
    "\n",
    "# Encode characters as indices.\n",
    "unique_chars = list(set(text))\n",
    "char_to_index = dict((ch, index) for index,\n",
    "                     ch in enumerate(unique_chars))\n",
    "index_to_char = dict((index, ch) for index,\n",
    "                     ch in enumerate(unique_chars))\n",
    "encoding_width = len(char_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create training examples from the text file. This is done by the next code snippet. Each training example will consist of a sequence of characters and a target output value of a single character immediately following the input characters. We create these input examples using a sliding window of length WINDOW_LENGTH. Once we have created one training example, we slide the window by WINDOW_STEP positions and create the next training example. We add the input examples to one list and the output values to another. All of this is done by the first for loop.\n",
    "\n",
    "We then create a single array holding all the input examples and another array holding the output values. Both of these arrays will hold data in one-hot encoded form, so each character is represented by a dimension of size encoding_width. We first allocate space for the two arrays and then fill in the values using a nested for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples.\n",
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "    targets.append(text[i + WINDOW_LENGTH])\n",
    "\n",
    "# Convert to one-hot encoded training data.\n",
    "X = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width), dtype=np.float32)\n",
    "y = np.zeros(len(fragments), dtype=np.int64)\n",
    "for i, fragment in enumerate(fragments):\n",
    "    for j, char in enumerate(fragment):\n",
    "        X[i, j, char_to_index[char]] = 1\n",
    "    target_char = targets[i]\n",
    "    y[i] = char_to_index[target_char]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the dataset into a training dataset and a test dataset using the train_test_split function from scikit-learn. We only withhold 5% as test dataset. Given that we are mostly interested in inspecting the resulting auto-completions in this example we could have skipped creating a test dataset altogether, but we create it anyway for good practice.\n",
    "\n",
    "We then convert the arrays into tensors and create Dataset objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set.\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=0)\n",
    "\n",
    "# Create Dataset objects.\n",
    "trainset = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "testset = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build our model. From the perspective of training our model, it will look similar to the book sales prediction example, but we use a deeper model consisting of two LSTM layers (indicated by the argument num_layers to the nn.LSTM object). We want both LSTM layers to use a dropout value of 0.2. However, the nn.LSTM implementation does not apply dropout to the top layer, so we stack a separate Dropout layer on top of the LSTM object.\n",
    "\n",
    "We have to create a custom layer to retrieve the correct set of outputs from the nn.LSTM object. The return value from LSTM is slightly more complex in that it returns both the internal cell state as well as the output state of each layer so we now need yet another index to select only the output state. That is, the second index (0) indicates that we want the output of the layer instead of the cell state, and the third index (1) indicates that we want the output of the second LSTM layer.\n",
    "\n",
    "We end with a fully connected layer with multiple neurons using a softmax function because we will be predicting probabilities for discrete entities (characters). We use categorical cross-entropy as our loss function, which is the recommended loss function for multicategory classification.\n",
    "\n",
    "Finally, we train the model for 32 epochs with a mini-batch size of 256.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model.\n",
    "class LastTimestep(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        return inputs[1][0][1] # Return hidden state for last timestep.\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.LSTM(encoding_width, 128, num_layers=2, dropout=0.2, batch_first=True),\n",
    "    LastTimestep(),\n",
    "    nn.Dropout(0.2), # Add this since PyTorch LSTM does not apply dropout to top layer.\n",
    "    nn.Linear(128, encoding_width)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model.\n",
    "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset,\n",
    "            optimizer, loss_function, 'acc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the beam search algorithm to predict text. In our implementation, each beam is represented by a tuple with three elements. The first element is the logarithm of the cumulative probability for the current sequence of characters. The second element is the string of characters. The third element is a one-hot encoded version of the string of characters. A reasonable question is why we store the logarithm of the cumulative probability instead of just the cumulative probability. Given that these probabilities are small, there is a risk that the limited precision of computer arithmetic results in underflow. This is addressed by instead computing the logarithm of the probability, in which case the multiplication is converted to an addition. For a small number of words, this is not necessary, but we do it anyway for good practice.\n",
    "\n",
    "We start by creating a single beam with an initial sequence of characters ('the body ') and set the initial probability to 1.0. The one-hot encoded version of the string is created by the first loop. We add this beam to a list named beams.\n",
    "\n",
    "This is followed by a nested loop that uses the trained model to do predictions according to the beam-search algorithm. We extract the one-hot encoding representation of each beam and create a NumPy array with multiple input examples. There is one input example per beam. During the first iteration, there is only a single input example. During the remaining iterations, there will be BEAM_SIZE number of examples.\n",
    "\n",
    "We convert the input to a tensor, move to the GPU and feed to the model. We also need to explicitly apply softmax to the output because the softmax operation is not included in the model itself. This results in one softmax vector per beam. The softmax vector contains one probability per word in the vocabulary. For each beam, we create BEAM_SIZE new beams, each beam consisting of the words from the original beam concatenated with one more word. We choose the most probable words when creating the beams. The probability for each beam can be computed by multiplying the current probability of the beam by the probability for the added word.\n",
    "\n",
    "Once we have created BEAM_SIZE beams for each existing beam, we sort the list of new beams according to their probabilities. We then discard all but the top BEAM_SIZE beams. This represents the pruning step. For the first iteration, this does not result in any pruning because we started with a single beam, and this beam resulted in just BEAM_SIZE beams. For all remaining iterations, we will end up with BEAM_SIZE * BEAM_SIZE beams and discard most of them.\n",
    "\n",
    "The loop runs for a fixed number of iterations followed by printing out the generated predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial single beam represented by triplet\n",
    "# (probability , string , one-hot encoded string).\n",
    "letters = 'the monster '\n",
    "one_hots = []\n",
    "for i, char in enumerate(letters):\n",
    "    x = np.zeros(encoding_width)\n",
    "    x[char_to_index[char]] = 1\n",
    "    one_hots.append(x)\n",
    "beams = [(np.log(1.0), letters, one_hots)]\n",
    "\n",
    "# Predict NUM_LETTERS into the future.\n",
    "for i in range(NUM_LETTERS):\n",
    "    minibatch_list = []\n",
    "    # Create minibatch from one-hot encodings, and predict.\n",
    "    for triple in beams:\n",
    "        minibatch_list.append(triple[2])\n",
    "    minibatch = np.array(minibatch_list, dtype=np.float32)\n",
    "    inputs = torch.from_numpy(minibatch)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    outputs = F.softmax(outputs, dim = 1)\n",
    "    y_predict = outputs.cpu().detach().numpy()\n",
    "\n",
    "    new_beams = []\n",
    "    for j, softmax_vec in enumerate(y_predict):\n",
    "        triple = beams[j]\n",
    "        # Create BEAM_SIZE new beams from each existing beam.\n",
    "        for k in range(BEAM_SIZE):\n",
    "            char_index = np.argmax(softmax_vec)\n",
    "            new_prob = triple[0] + np.log(softmax_vec[char_index])\n",
    "            new_letters = triple[1] + index_to_char[char_index]\n",
    "            x = np.zeros(encoding_width)\n",
    "            x[char_index] = 1\n",
    "            new_one_hots = triple[2].copy()\n",
    "            new_one_hots.append(x)\n",
    "            new_beams.append((new_prob, new_letters, new_one_hots))\n",
    "            softmax_vec[char_index] = 0\n",
    "    # Prune tree to only keep BEAM_SIZE most probable beams.\n",
    "    new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    beams = new_beams[0:BEAM_SIZE]\n",
    "for item in beams:\n",
    "    print(item[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt210_py3100)",
   "language": "python",
   "name": "pt210_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
