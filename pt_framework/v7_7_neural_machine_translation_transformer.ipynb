{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to build a neural machine translation network. It is a sequence-to-sequence network based on a Transformer encoder-decoder architecture. More context for this code example can be found in video 7.7 \"Programming Example: Machine Translation Using Transformer with PyTorch\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n",
    "\n",
    "The data used to train the model is expected to be in the file ../data/fra.txt.\n",
    "We begin by importing modules that we need for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence \\\n",
    "    import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some constants. We specify a vocabulary size of 10,000 symbols, out of which four indices are reserved for padding, out-of-vocabulary words (denoted as UNK), START tokens, and STOP tokens. Our training corpus is large, so we set the parameter READ_LINES to the number of lines in the input file we want to use in our example (60,000). The parameter LAYER_SIZE defines the width of the intermediate fully-connected layer in the Transformer, and the embedding layers output 128 dimensions (EMBEDDING_WIDTH). We use 20% (TEST_PERCENT) of the dataset as test set and further select 20 sentences (SAMPLE_SIZE) to inspect in detail during training. We limit the length of the source and destination sentences to, at most, 60 words (MAX_LENGTH). Finally, we provide the path to the data file, where each line is expected to contain two versions of the same sentence (one in each language) separated by a tab character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "MAX_WORDS = 10000\n",
    "READ_LINES = 60000\n",
    "NUM_HEADS = 8\n",
    "LAYER_SIZE = 256\n",
    "EMBEDDING_WIDTH = 128\n",
    "TEST_PERCENT = 0.2\n",
    "SAMPLE_SIZE = 20\n",
    "OOV_WORD = 'UNK'\n",
    "PAD_INDEX = 0\n",
    "OOV_INDEX = 1\n",
    "START_INDEX = MAX_WORDS - 2\n",
    "STOP_INDEX = MAX_WORDS - 1\n",
    "MAX_LENGTH = 60\n",
    "SRC_DEST_FILE_NAME = '../data/fra.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the function used to read the input data file and do some initial processing. Each line is split into two strings, where the first contains the sentence in the destination language and the second contains the sentence in the source language. We use the function text_to_word_sequence() to clean the data somewhat (make everything lowercase and remove punctuation) and split each sentence into a list of individual words. If the list (sentence) is longer than the maximum allowed length, then it is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read file.\n",
    "def read_file_combined(file_name, max_len):\n",
    "    file = open(file_name, 'r', encoding='utf-8')\n",
    "    src_word_sequences = []\n",
    "    dest_word_sequences = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i == READ_LINES:\n",
    "            break\n",
    "        pair = line.split('\\t')\n",
    "        word_sequence = text_to_word_sequence(pair[1])\n",
    "        src_word_sequence = word_sequence[0:max_len]\n",
    "        src_word_sequences.append(src_word_sequence)\n",
    "        word_sequence = text_to_word_sequence(pair[0])\n",
    "        dest_word_sequence = word_sequence[0:max_len]\n",
    "        dest_word_sequences.append(dest_word_sequence)\n",
    "    file.close()\n",
    "    return src_word_sequences, dest_word_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows functions used to turn sequences of words into\n",
    "sequences of tokens, and vice versa. We call tokenize() a single time for each\n",
    "language, so the argument sequences is a list of lists where each of the inner\n",
    "lists represents a sentence. The Tokenizer class assigns indices to the most\n",
    "common words and returns either these indices or the reserved OOV_INDEX\n",
    "for less common words that did not make it into the vocabulary. We tell the\n",
    "Tokenizer to use a vocabulary of 9998 (MAX_WORDS-2)—that is, use only\n",
    "indices 0 to 9997, so that we can use indices 9998 and 9999 as our START and\n",
    "STOP tokens (the Tokenizer does not support the notion of START and STOP\n",
    "tokens but does reserve index 0 to use as a padding token and index 1 for outof-\n",
    "vocabulary words). Our tokenize() function returns both the tokenized\n",
    "sequence and the Tokenizer object itself. This object will be needed anytime we\n",
    "want to convert tokens back into words.\n",
    "\n",
    "The function tokens_to_words() requires a Tokenizer and a list of indices. We simply check for the reserved indices: If we find a match, we replace them with hardcoded strings, and if we find no match, we let the Tokenizer convert the index to the corresponding word string. The Tokenizer expects a list of lists of indices and returns a list of strings, which is why we need to call it with [[index]] and then select the 0th element to arrive at a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to tokenize and un-tokenize sequences.\n",
    "def tokenize(sequences):\n",
    "    # \"MAX_WORDS-2\" used to reserve two indices\n",
    "    # for START and STOP.\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n",
    "                          oov_token=OOV_WORD)\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    token_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    return tokenizer, token_sequences\n",
    "\n",
    "def tokens_to_words(tokenizer, seq):\n",
    "    word_seq = []\n",
    "    for index in seq:\n",
    "        if index == PAD_INDEX:\n",
    "            word_seq.append('PAD')\n",
    "        elif index == OOV_INDEX:\n",
    "            word_seq.append(OOV_WORD)\n",
    "        elif index == START_INDEX:\n",
    "            word_seq.append('START')\n",
    "        elif index == STOP_INDEX:\n",
    "            word_seq.append('STOP')\n",
    "        else:\n",
    "            word_seq.append(tokenizer.sequences_to_texts(\n",
    "                [[index]])[0])\n",
    "    print(word_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these helper functions, it is trivial to read the input data\n",
    "file and convert into tokenized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and tokenize.\n",
    "src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME,\n",
    "                                       MAX_LENGTH)\n",
    "src_tokenizer, src_token_seq = tokenize(src_seq)\n",
    "dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to arrange the data into arrays that can be used for training and testing. The following example provides some insight into what we need as input and output for a single training example, where src_input is the input to the encoder network, dest_input is the input to the decoder network, and dest_target is the desired output from the decoder network:\n",
    "\n",
    "src_input = [PAD, PAD, PAD, id(\"je\"), id(\"suis\"), id(\"étudiant\")]\n",
    "dest_input = [START, id(\"i\"), id(\"am\"), id(\"a\"), id(\"student\"), STOP, PAD, PAD]\n",
    "dest_target = [one_hot_id(\"i\"), one_hot_id(\"am\"), one_hot_id(\"a\"), one_hot_id(\"student\"), one_hot_id(STOP), one_hot_id(PAD), one_hot_id(PAD), one_hot_id(PAD)]\n",
    "\n",
    "In the example, id(string) refers to the tokenized index of the string, and one_hot_id is the one-hot encoded version of the index. We have assumed that the longest source sentence is six words, so we padded src_input to be of that length. Similarly, we have assumed that the longest destination sentence is eight words including START and STOP tokens, so we padded both dest_input and dest_target to be of that length. Note how the symbols in dest_input are offset by one location compared to the symbols in dest_target because when we later do inference, the inputs into the decoder network will be coming from the output of the network for the previous timestep. Although this example has shown the training example as being lists, in reality, they will be rows in NumPy arrays, where each array contains multiple training examples.\n",
    "\n",
    "The padding is done to ensure that we can use mini-batches for training. That is, all source sentences need to be the same length, and all destination sentences need to be the same length. We pad the source input at the beginning (known as prepadding) and the destination at the end (known as postpadding).\n",
    "\n",
    "The code snippet below shows a compact way of creating the three arrays that we need. The first two lines create two new lists, each containing the destination sequences but the first (dest_target_token_seq) also augmented with STOP_INDEX after each sequence and the second (dest_input_token_seq) augmented with both START_INDEX and STOP_INDEX. It is easy to miss that dest_input_token_seq has a STOP_INDEX, but that falls out naturally because it is created from the dest_target_token_seq for which a STOP_INDEX was just added to each sentence.\n",
    "\n",
    "Next, we call pad_sequences() on both the original src_input_data list (of lists) and on these two new destination lists. The pad_sequences() function pads the sequences with the PAD value and then returns a NumPy array. The default behavior of pad_sequences is to do prepadding, and we do that for the source sequence but explicitly ask for postpadding for the destination sequences.\n",
    "\n",
    "We conclude with converting the data type to np.int64 to match what PyTorch later requires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n",
    "dest_input_token_seq = [[START_INDEX] + x for x in\n",
    "                        dest_target_token_seq]\n",
    "src_input_data = pad_sequences(src_token_seq)\n",
    "dest_input_data = pad_sequences(dest_input_token_seq,\n",
    "                                padding='post')\n",
    "dest_target_data = pad_sequences(\n",
    "    dest_target_token_seq, padding='post', maxlen\n",
    "    = len(dest_input_data[0]))\n",
    "\n",
    "# Convert to same precision as model.\n",
    "src_input_data = src_input_data.astype(np.int64)\n",
    "dest_input_data = dest_input_data.astype(np.int64)\n",
    "dest_target_data = dest_target_data.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet demonstrates how we can manually split our dataset into a training dataset and a test dataset. We split the dataset by first creating a list test_indices, which contains a 20% (TEST_PERCENT) subset of all the numbers from 0 to N−1, where N is the size of our original dataset. We then create a list train_indices, which contains the remaining 80%. We can now use these lists to select a number of rows in the arrays representing the dataset and create two new collections of arrays, one to be used as training set and one to be used as test set. Finally, we create a third collection of arrays, which only contains 20 (SAMPLE_SIZE) random examples from the test dataset. We will use them to inspect the resulting translations in detail, but since that is a manual process, we limit ourselves to a small number of sentences.\n",
    "\n",
    "Finally, we convert the NumPy arrays to PyTorch tensors and create Dataset objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set.\n",
    "rows = len(src_input_data[:,0])\n",
    "all_indices = list(range(rows))\n",
    "test_rows = int(rows * TEST_PERCENT)\n",
    "test_indices = random.sample(all_indices, test_rows)\n",
    "train_indices = [x for x in all_indices if x not in test_indices]\n",
    "\n",
    "train_src_input_data = src_input_data[train_indices]\n",
    "train_dest_input_data = dest_input_data[train_indices]\n",
    "train_dest_target_data = dest_target_data[train_indices]\n",
    "\n",
    "test_src_input_data = src_input_data[test_indices]\n",
    "test_dest_input_data = dest_input_data[test_indices]\n",
    "test_dest_target_data = dest_target_data[test_indices]\n",
    "\n",
    "# Create a sample of the test set that we will inspect in detail.\n",
    "test_indices = list(range(test_rows))\n",
    "sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n",
    "sample_input_data = test_src_input_data[sample_indices]\n",
    "sample_target_data = test_dest_target_data[sample_indices]\n",
    "\n",
    "# Create Dataset objects.\n",
    "trainset = TensorDataset(torch.from_numpy(train_src_input_data),\n",
    "                         torch.from_numpy(train_dest_input_data),\n",
    "                         torch.from_numpy(train_dest_target_data))\n",
    "testset = TensorDataset(torch.from_numpy(test_src_input_data),\n",
    "                         torch.from_numpy(test_dest_input_data),\n",
    "                         torch.from_numpy(test_dest_target_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide ordering information between the embeddings we need to add positional encodings to each embedding. We do this by creating a class PositionalEncoding that takes an embedding as input and adds the positional encoding. \n",
    "\n",
    "We calculate the positional encoding using sine and cosine as in the original Transformer paper and add it to the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        i_range = np.arange(d_model).reshape(1, d_model)\n",
    "        pos_range = np.arange(max_len).reshape(max_len, 1)\n",
    "        sine_matrix = np.sin(1 / np.power(10000, i_range/d_model) * pos_range)\n",
    "        cosine_matrix = np.cos(1 / np.power(10000, (i_range-1)/d_model) * pos_range)\n",
    "        pos_matrix = np.zeros((max_len, d_model))\n",
    "        for i in range(d_model):\n",
    "            if (i % 2 == 0):\n",
    "                pos_matrix[:, i] = sine_matrix[:, i]\n",
    "            else:\n",
    "                pos_matrix[:, i] = cosine_matrix[:, i]\n",
    "        pos_matrix = pos_matrix.reshape(1, max_len, d_model).astype(np.float32)\n",
    "        self.pos_matrix = torch.from_numpy(pos_matrix).to(device)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.pos_matrix[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build our model. It consists of an encoder part and a decoder part. The encoder consists of an Embedding layer, a PositionalEncoding layer, and two Transformer encoder modules stacked on top of each other. The decoder consists of an Embedding layer, a PositionalEncoding layer, two Transformer decoder modules stacked on top of each other, and a fully connected softmax layer. We define these as two separate models, but we will use them together as an encoder-decoder model.\n",
    "\n",
    "The code snippet below contains the implementation of the encoder model. The way to define the Transformer encoder modules in PyTorch is to first define a TransformerEncoderLayer object with the desired parameters, and then pass that as input to the constructor of a TransformerEncoder object that creates multiple instances stacked on top of each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models.\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(MAX_WORDS, EMBEDDING_WIDTH)\n",
    "        self.positional_layer = PositionalEncoding(EMBEDDING_WIDTH, MAX_LENGTH)\n",
    "        enc_layer = TransformerEncoderLayer(EMBEDDING_WIDTH, NUM_HEADS, LAYER_SIZE, batch_first=True)\n",
    "        self.trans_enc_layers = TransformerEncoder(enc_layer, 2)\n",
    "        nn.init.uniform_(self.embedding_layer.weight, -0.05, 0.05) # Default is -1, 1.\n",
    "\n",
    "    def forward(self, inputs, pad_mask = None):\n",
    "        x = self.embedding_layer(inputs) * math.sqrt(EMBEDDING_WIDTH)\n",
    "        x = self.positional_layer(x)\n",
    "        x = self.trans_enc_layers(x, src_key_padding_mask=pad_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows the implementation of the decoder model. In addition to the sentence in the destination language, it needs the output state from the encoder model. It also requires a mask that defines which positions a give position is allowed to self-attend to. In our use-case it is simple in that each position is only allowed to attend to prior positions, but the PyTorch implementation allows flexibility to have more complicated relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(MAX_WORDS, EMBEDDING_WIDTH)\n",
    "        self.positional_layer = PositionalEncoding(EMBEDDING_WIDTH, MAX_LENGTH)\n",
    "        dec_layer = TransformerDecoderLayer(EMBEDDING_WIDTH, NUM_HEADS, LAYER_SIZE, batch_first=True)\n",
    "        self.trans_dec_layers = TransformerDecoder(dec_layer, 2)\n",
    "        nn.init.uniform_(self.embedding_layer.weight, -0.05, 0.05) # Default is -1, 1.\n",
    "        self.output_layer = nn.Linear(EMBEDDING_WIDTH, MAX_WORDS)\n",
    "\n",
    "    def forward(self, embedding_inputs, state_inputs, causal_mask,  pad_mask = None):\n",
    "        x = self.embedding_layer(embedding_inputs) * math.sqrt(EMBEDDING_WIDTH)\n",
    "        x = self.positional_layer(x)\n",
    "        x = self.trans_dec_layers(x, state_inputs, tgt_mask = causal_mask,\n",
    "                                  tgt_is_causal=True, tgt_key_padding_mask=pad_mask)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet instantitates the two models, and creates two optimizers, one for each model. We decided to use RMSProp as optimizer because some experiments indicate that it performs better than Adam for this specific model. We use CrossEntropyLoss as usual.\n",
    "\n",
    "We transfer the models to the GPU and create a DataLoader object for both the training and test dataset. We have not had to do this lately because it has been included in our train_model funtion that was reused for all recent examples. We cannot use that function in this example because it does not support the more complex encoder-decoder model that we want to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = EncoderModel()\n",
    "decoder_model = DecoderModel()\n",
    "\n",
    "# Loss functions and optimizer.\n",
    "encoder_optimizer = torch.optim.RMSprop(encoder_model.parameters(), lr=0.001)\n",
    "decoder_optimizer = torch.optim.RMSprop(decoder_model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Using a custom training loop instead of our standard training function.\n",
    "# Transfer model to GPU.\n",
    "encoder_model.to(device)\n",
    "decoder_model.to(device)\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final code snippet shows hos to train and test the model. It is very similar to the LSTM-based translation network. The key difference is the mask objects needed by the Transformer encoder and decoder modules. Both the encoder and decoder require a mask indicated, which input positions correspond to PAD tokens. Additionally, the decoder requires a mask to specify which positions it is allowed to self-attend to. This mask is created with the method generate_square_subsequent_mask(), which is provided by the Transformer class.\n",
    "\n",
    "The third inner loop that does autoregression is also modified. We provide the source sentence to the encoder model to create the resulting internal state and store to the variable intermediate_states. We then set the input x to the START token and use the decoder to make a prediction. We retrieve the most probable word and append it to x. We then provide this sequence to the decoder and make a new prediction. We iterate this with a gradually growing input sequence in an autoregressive manner until the model produces a STOP token or until a given number of words have been produced. Finally, we convert the produced tokenized sequences into the corresponding word sequences and print them out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test repeatedly.\n",
    "for i in range(EPOCHS):\n",
    "    encoder_model.train() # Set model in training mode.\n",
    "    decoder_model.train() # Set model in training mode.\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_batches = 0\n",
    "    train_elems = 0\n",
    "    for src_inputs, dest_inputs, dest_targets in trainloader:\n",
    "        # Move data to GPU.\n",
    "        src_inputs, dest_inputs, dest_targets = src_inputs.to(\n",
    "            device), dest_inputs.to(device), dest_targets.to(device)\n",
    "        \n",
    "        # Create masks\n",
    "        decode_input_width = dest_inputs.shape[1]\n",
    "        decoder_causal_mask = Transformer.generate_square_subsequent_mask(\n",
    "            decode_input_width, device=device, dtype=torch.float32)\n",
    "        encoder_pad_mask = (src_inputs == PAD_INDEX)\n",
    "        decoder_pad_mask = (dest_inputs == PAD_INDEX)\n",
    "\n",
    "        # Zero the parameter gradients.\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        encoder_state = encoder_model(src_inputs, encoder_pad_mask)\n",
    "        outputs = decoder_model(dest_inputs, encoder_state,\n",
    "                                decoder_causal_mask, decoder_pad_mask)\n",
    "        loss = loss_function(outputs.view(-1, MAX_WORDS), dest_targets.view(-1))\n",
    "        # Accumulate metrics.\n",
    "        _, indices = torch.max(outputs.data, 2)\n",
    "        train_correct += (indices == dest_targets).sum().item()\n",
    "        train_elems += indices.numel()\n",
    "        train_batches +=  1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update.\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / train_batches\n",
    "    train_acc = train_correct / train_elems\n",
    "\n",
    "    # Evaluate the model on the test dataset.\n",
    "    encoder_model.eval() # Set model in inference mode.\n",
    "    decoder_model.eval() # Set model in inference mode.\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_batches = 0\n",
    "    test_elems = 0\n",
    "    for src_inputs, dest_inputs, dest_targets in testloader:\n",
    "        # Move data to GPU.\n",
    "        src_inputs, dest_inputs, dest_targets = src_inputs.to(\n",
    "            device), dest_inputs.to(device), dest_targets.to(device)\n",
    "\n",
    "        # Create masks\n",
    "        decode_input_width = dest_inputs.shape[1]\n",
    "        decoder_causal_mask = Transformer.generate_square_subsequent_mask(\n",
    "            decode_input_width, device=device, dtype=torch.float32)\n",
    "        encoder_pad_mask = (src_inputs == PAD_INDEX)\n",
    "        decoder_pad_mask = (dest_inputs == PAD_INDEX)\n",
    "                \n",
    "        encoder_state = encoder_model(src_inputs, encoder_pad_mask)\n",
    "        outputs = decoder_model(dest_inputs, encoder_state,\n",
    "                                decoder_causal_mask, decoder_pad_mask)\n",
    "        loss = loss_function(outputs.view(-1, MAX_WORDS), dest_targets.view(-1))\n",
    "        _, indices = torch.max(outputs, 2)\n",
    "        test_correct += (indices == dest_targets).sum().item()\n",
    "        test_elems += indices.numel()\n",
    "        test_batches +=  1\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss = test_loss / test_batches\n",
    "    test_acc = test_correct / test_elems\n",
    "    print(f'Epoch {i+1}/{EPOCHS} loss: {train_loss:.4f} - acc: {train_acc:0.4f} - val_loss: {test_loss:.4f} - val_acc: {test_acc:0.4f}')\n",
    "\n",
    "    # Loop through samples to see result\n",
    "    for (test_input, test_target) in zip(sample_input_data,\n",
    "                                         sample_target_data):\n",
    "        # Run a single sentence through encoder model.\n",
    "        x = np.reshape(test_input, (1, -1))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        inputs = inputs.to(device)\n",
    "        intermediate_states = encoder_model(inputs)\n",
    "\n",
    "        # Provide resulting state and START_INDEX as input\n",
    "        # to decoder model.\n",
    "        x = np.reshape(np.array(START_INDEX), (1, 1))\n",
    "        produced_string = ''\n",
    "        pred_seq = []\n",
    "        for j in range(MAX_LENGTH):\n",
    "            # Predict next word and capture internal state.\n",
    "            decode_input_width = x.shape[1]\n",
    "            decoder_causal_mask = Transformer.generate_square_subsequent_mask(\n",
    "                decode_input_width, device=device, dtype=torch.float32)\n",
    "            inputs = torch.from_numpy(x)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = decoder_model(inputs, intermediate_states, decoder_causal_mask)\n",
    "            preds = outputs.cpu().detach().numpy()[0][j]\n",
    "\n",
    "            # Find the most probable word.\n",
    "            word_index = preds.argmax()\n",
    "            pred_seq.append(word_index)\n",
    "            if word_index == STOP_INDEX:\n",
    "                break\n",
    "            x = np.append(x, [[word_index]], axis=1)\n",
    "        tokens_to_words(src_tokenizer, test_input)\n",
    "        tokens_to_words(dest_tokenizer, test_target)\n",
    "        tokens_to_words(dest_tokenizer, pred_seq)\n",
    "        print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt210_py3100)",
   "language": "python",
   "name": "pt210_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
