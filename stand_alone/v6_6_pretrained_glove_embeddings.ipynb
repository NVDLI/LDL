{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example explores properties of GloVe word embeddings and word vector arithmetics. More context for this code example can be found in video 6.6 \"Programming Example: Using Pretrained GloVe Embeddings\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code snippet contains two import statements and a function to read the embeddings. The function simply opens the file and reads it line by line. It splits each line into its elements. It extracts the first element, which represents the word itself, and then creates a vector from the remaining elements and inserts the word and the corresponding vector into a dictionary, which serves as the return value of the function. The embeddings are assumed to be in the file ../data/glove.6B.100d.txt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial\n",
    "\n",
    "# Read embeddings from file.\n",
    "def read_embeddings():\n",
    "    FILE_NAME = '../data/glove.6B.100d.txt'\n",
    "    embeddings = {}\n",
    "    file = open(FILE_NAME, 'r', encoding='utf-8')\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:],\n",
    "                            dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    file.close()\n",
    "    print('Read %s embeddings.' % len(embeddings))\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet implements a function that computes the cosine distance between a specific embedding and all other embeddings. It then prints the n closest ones. Euclidean distance would also have worked fine, but the results would sometimes be different because the GloVe vectors are not normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_closest(embeddings, vec0, n):\n",
    "    word_distances = {}\n",
    "    for (word, vec1) in embeddings.items():\n",
    "        distance = scipy.spatial.distance.cosine(\n",
    "            vec1, vec0)\n",
    "        word_distances[distance] = word\n",
    "    # Print words sorted by distance.\n",
    "    for distance in sorted(word_distances.keys())[:n]:\n",
    "        word = word_distances[distance]\n",
    "        print(word + ': %6.3f' % distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these two functions, we can now retrieve word embeddings for arbitrary words and print out words that have similar embeddings. This is shown below, where we first read call read_embeddings() and then retrieve the embeddings for hello, precisely, and dog and call print_n_closest() on each of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings()\n",
    "\n",
    "lookup_word = 'hello'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings,\n",
    "                embeddings[lookup_word], 3)\n",
    "\n",
    "lookup_word = 'precisely'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings,\n",
    "                embeddings[lookup_word], 3)\n",
    "\n",
    "lookup_word = 'dog'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings,\n",
    "                embeddings[lookup_word], 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NumPy, it is also trivial to combine multiple vectors using vector arithmetic and then print out words that are similar to the resulting vector. This is demonstrated in the code snippet below, which first prints the words closest to the word vector for king and then prints the words closest to the vector resulting from computing (king âˆ’ man + woman).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_word = 'king'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings,\n",
    "                embeddings[lookup_word], 3)\n",
    "\n",
    "lookup_word = '(king - man + woman)'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "vec = embeddings['king'] - embeddings[\n",
    "    'man'] + embeddings['woman']\n",
    "print_n_closest(embeddings, vec, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example is shown below where we print the vector resulting from subtracting Spain and adding Sweden to the word Madrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_word = 'sweden'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings,\n",
    "                embeddings[lookup_word], 3)\n",
    "\n",
    "lookup_word = 'madrid'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings,\n",
    "                embeddings[lookup_word], 3)\n",
    "\n",
    "lookup_word = '(madrid - spain + sweden)'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "vec = embeddings['madrid'] - embeddings[\n",
    "    'spain'] + embeddings['sweden']\n",
    "print_n_closest(embeddings, vec, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pt210_py3100)",
   "language": "python",
   "name": "pt210_py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
