{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example trains a two-level network to classify handwritten digits using the MNIST dataset. More context for this code example can be found in the section \"Programming Example: Classifying Handwritten Digits\" in Chapter 4 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is a modified version of the implementation of the XOR learning example in c3e1_2level_learning_xor. One difference is that the first code snippet contains some initializations where we now provide paths to the training and test datasets instead of defining the training values as hardcoded variables. We also tweaked the learning rate to 0.01 and introduced a parameter EPOCHS. The dataset is assumed to be in the directory ../data/mnist/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "\n",
    "np.random.seed(7) # To make repeatable\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 20\n",
    "TRAIN_IMAGE_FILENAME = '../data/mnist/train-images-idx3-ubyte'\n",
    "TRAIN_LABEL_FILENAME = '../data/mnist/train-labels-idx1-ubyte'\n",
    "TEST_IMAGE_FILENAME = '../data/mnist/t10k-images-idx3-ubyte'\n",
    "TEST_LABEL_FILENAME = '../data/mnist/t10k-labels-idx1-ubyte'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also added a function to read the datasets from files, as shown in the next code snippet. To simplify feeding the input data to the network, we reshape the images from two dimensions into a single dimension. That is, the arrays of images are now 2D instead of 3D. After this, we scale the pixel values and center them around 0. This is known as standardizing the data. We first compute the mean and standard deviation of all the training values. We standardize the data by subtracting the mean from each pixel value and dividing by the standard deviation.\n",
    "\n",
    "The next step is to one-hot encode the digit number to be used as a ground truth for our ten-output network. We one-hot encode by creating an array of ten numbers, each being 0 (using the NumPy zeros function), and then set one of them to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read dataset.\n",
    "def read_mnist():\n",
    "    train_images = idx2numpy.convert_from_file(\n",
    "        TRAIN_IMAGE_FILENAME)\n",
    "    train_labels = idx2numpy.convert_from_file(\n",
    "        TRAIN_LABEL_FILENAME)\n",
    "    test_images = idx2numpy.convert_from_file(\n",
    "        TEST_IMAGE_FILENAME)\n",
    "    test_labels = idx2numpy.convert_from_file(\n",
    "        TEST_LABEL_FILENAME)\n",
    "\n",
    "    # Reformat and standardize.\n",
    "    x_train = train_images.reshape(60000, 784)\n",
    "    mean = np.mean(x_train)\n",
    "    stddev = np.std(x_train)\n",
    "    x_train = (x_train - mean) / stddev\n",
    "    x_test = test_images.reshape(10000, 784)\n",
    "    x_test = (x_test - mean) / stddev\n",
    "\n",
    "    # One-hot encoded output.\n",
    "    y_train = np.zeros((60000, 10))\n",
    "    y_test = np.zeros((10000, 10))\n",
    "    for i, y in enumerate(train_labels):\n",
    "        y_train[i][y] = 1\n",
    "    for i, y in enumerate(test_labels):\n",
    "        y_test[i][y] = 1\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Read train and test examples.\n",
    "x_train, y_train, x_test, y_test = read_mnist()\n",
    "index_list = list(range(len(x_train))) # Used for random order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet contains the implementation of the layer weights and the instantiation of the network. It has 25 neurons in the hidden layer and 10 output neurons. Each neuron in the hidden layer will have 784 inputs + bias, and each neuron in the output layer will have 25 inputs + bias. The for loop that initializes the weights starts with j=1 and therefore does not initialize the bias weight but just leaves it at 0 as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_w(neuron_count, input_count):\n",
    "    weights = np.zeros((neuron_count, input_count+1))\n",
    "    for i in range(neuron_count):\n",
    "        for j in range(1, (input_count+1)):\n",
    "            weights[i][j] = np.random.uniform(-0.1, 0.1)\n",
    "    return weights\n",
    "\n",
    "# Declare matrices and vectors representing the neurons.\n",
    "hidden_layer_w = layer_w(25, 784)\n",
    "hidden_layer_y = np.zeros(25)\n",
    "hidden_layer_error = np.zeros(25)\n",
    "\n",
    "output_layer_w = layer_w(10, 25)\n",
    "output_layer_y = np.zeros(10)\n",
    "output_layer_error = np.zeros(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows two functions that are used to report progress and to visualize the learning process. The function show_learning is called multiple times during training; it simply prints the current training and test accuracy and stores these values in two arrays. The function plot_learning is called at the end of the program and uses the two arrays to plot the training and test error (1.0 minus accuracy) over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_x = []\n",
    "chart_y_train = []\n",
    "chart_y_test = []\n",
    "def show_learning(epoch_no, train_acc, test_acc):\n",
    "    global chart_x\n",
    "    global chart_y_train\n",
    "    global chart_y_test\n",
    "    print('epoch no:', epoch_no, ', train_acc: ',\n",
    "          '%6.4f' % train_acc,\n",
    "          ', test_acc: ', '%6.4f' % test_acc)\n",
    "    chart_x.append(epoch_no + 1)\n",
    "    chart_y_train.append(1.0 - train_acc)\n",
    "    chart_y_test.append(1.0 - test_acc)\n",
    "\n",
    "def plot_learning():\n",
    "    plt.plot(chart_x, chart_y_train, 'r-',\n",
    "             label='training error')\n",
    "    plt.plot(chart_x, chart_y_test, 'b-',\n",
    "             label='test error')\n",
    "    plt.axis([0, len(chart_x), 0.0, 1.0])\n",
    "    plt.xlabel('training epochs')\n",
    "    plt.ylabel('error')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet contains the functions for the forward and backward passes as well as for adjusting the weights. The forward_pass and backward_pass functions also implicitly define the topology of the network.\n",
    "\n",
    "The forward_pass function contains two loops. The first one loops over all hidden neurons and presents the same input (the pixels) to them all. It also collects all the outputs of the hidden neurons into an array together with a bias term that can then be used as input to the neurons in the output layer. Similarly, the second loop presents this input to each of the output neurons and collects all the outputs of the output layer into an array that is returned to the caller of the function.\n",
    "\n",
    "The backward_pass function is somewhat similar. It first loops through all the output neurons and computes the derivative of the loss function for each output neuron. In the same loop, it also computes the derivative of the activation function for each neuron. The error term for each neuron can now be calculated by multiplying the derivative of the loss function by the derivative of the activation function. The second loop in the function loops over all hidden neurons. For the hidden neurons, the error term is a little bit more complicated. It is computed as a weighted sum (computed as a dot product) of the backpropagated error from each of the output neurons, multiplied by the derivative of the activation function for the hidden neuron.\n",
    "\n",
    "The adjust_weights function is straightforward, where we again loop over each neuron in each layer and adjust the weights using the input values and error terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    global hidden_layer_y\n",
    "    global output_layer_y\n",
    "    # Activation function for hidden layer\n",
    "    for i, w in enumerate(hidden_layer_w):\n",
    "        z = np.dot(w, x)\n",
    "        hidden_layer_y[i] = np.tanh(z)\n",
    "    hidden_output_array = np.concatenate(\n",
    "        (np.array([1.0]), hidden_layer_y))\n",
    "    # Activation function for output layer\n",
    "    for i, w in enumerate(output_layer_w):\n",
    "        z = np.dot(w, hidden_output_array)\n",
    "        output_layer_y[i] = 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def backward_pass(y_truth):\n",
    "    global hidden_layer_error\n",
    "    global output_layer_error\n",
    "    # Backpropagate error for each output neuron\n",
    "    # and create array of all output neuron errors.\n",
    "    for i, y in enumerate(output_layer_y):\n",
    "        error_prime = -(y_truth[i] - y) # Loss derivative\n",
    "        derivative = y * (1.0 - y) # Logistic derivative\n",
    "        output_layer_error[i] = error_prime * derivative\n",
    "    for i, y in enumerate(hidden_layer_y):\n",
    "        # Create array weights connecting the output of\n",
    "        # hidden neuron i to neurons in the output layer.\n",
    "        error_weights = []\n",
    "        for w in output_layer_w:\n",
    "            error_weights.append(w[i+1])\n",
    "        error_weight_array = np.array(error_weights)\n",
    "        # Backpropagate error for hidden neuron.\n",
    "        derivative = 1.0 - y**2 # tanh derivative\n",
    "        weighted_error = np.dot(error_weight_array,\n",
    "                                output_layer_error)\n",
    "        hidden_layer_error[i] = weighted_error * derivative\n",
    "\n",
    "def adjust_weights(x):\n",
    "    global output_layer_w\n",
    "    global hidden_layer_w\n",
    "    for i, error in enumerate(hidden_layer_error):\n",
    "        hidden_layer_w[i] -= (x * LEARNING_RATE\n",
    "                              * error) # Update all weights\n",
    "    hidden_output_array = np.concatenate(\n",
    "        (np.array([1.0]), hidden_layer_y))\n",
    "    for i, error in enumerate(output_layer_error):\n",
    "        output_layer_w[i] -= (hidden_output_array\n",
    "                              * LEARNING_RATE\n",
    "                              * error) # Update all weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final code snippet shows the network training loop. We train for a fixed number of epochs. An epoch is defined as one iteration through all the training data. For each training example, we do a forward pass followed by a backward pass, and then we adjust the weights. We also track how many of the training examples were correctly predicted. We then loop through all the test examples and just record how many were correctly predicted. We use the NumPy argmax function to identify the array index corresponding to the greatest value; this decodes our one-hot encoded vector into an integer number. Before passing the input examples to forward_pass and adjust_weights, we extend each array with a leading 1.0 because these functions expect a bias term of 1.0 as the first entry in the array.\n",
    "\n",
    "We do not do any backward pass or weight adjustments for the test data. The reason for this is that we are not allowed to train on the test data because that will result in an optimistic assessment of how well the network works. At the end of each epoch, we print out the current accuracy for both the training data and the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network training loop.\n",
    "for i in range(EPOCHS): # Train EPOCHS iterations\n",
    "    np.random.shuffle(index_list) # Randomize order\n",
    "    correct_training_results = 0\n",
    "    for j in index_list: # Train on all examples\n",
    "        x = np.concatenate((np.array([1.0]), x_train[j]))\n",
    "        forward_pass(x)\n",
    "        if output_layer_y.argmax() == y_train[j].argmax():\n",
    "            correct_training_results += 1\n",
    "        backward_pass(y_train[j])\n",
    "        adjust_weights(x)\n",
    "\n",
    "    correct_test_results = 0\n",
    "    for j in range(len(x_test)): # Evaluate network\n",
    "        x = np.concatenate((np.array([1.0]), x_test[j]))\n",
    "        forward_pass(x)\n",
    "        if output_layer_y.argmax() == y_test[j].argmax():\n",
    "            correct_test_results += 1\n",
    "    # Show progress.\n",
    "    show_learning(i, correct_training_results/len(x_train),\n",
    "                  correct_test_results/len(x_test))\n",
    "plot_learning() # Create plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
