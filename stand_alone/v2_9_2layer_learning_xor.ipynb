{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example implements back-propagation algorithm for a two-layer network and demonstrates how to use it to learn the exclusive OR (XOR) function. The network has two neurons in the hidden layer and a single output neuron. More context for this code example can be found in video 2.9 \"Programming Example: Learning the XOR Function\" in the video series \"Learning Deep Learning: From Perceptron to Large Language Models\" by Magnus Ekman (Video ISBN-13: 9780138177614).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization code in the code snippet below is similar to what we did for the perceptron example v2_2_perceptron_learning. One thing to note is that we have started to use NumPy arrays so that we can make use of some NumPy functionality. The same holds for our random number generator (we call np.random.seed instead of just random.seed).\n",
    "\n",
    "For the training examples, we have now changed the ground truth to be between 0.0 and 1.0 because we have decided to use the logistic sigmoid function as an activation function for the output neuron, and its output range does not go to âˆ’1.0 as the perceptron did.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(3) # To make repeatable\n",
    "LEARNING_RATE = 0.1\n",
    "index_list = [0, 1, 2, 3] # Used to randomize order\n",
    "\n",
    "# Define training examples.\n",
    "x_train = [np.array([1.0, -1.0, -1.0]),\n",
    "           np.array([1.0, -1.0, 1.0]),\n",
    "           np.array([1.0, 1.0, -1.0]),\n",
    "           np.array([1.0, 1.0, 1.0])]\n",
    "y_train = [0.0, 1.0, 1.0, 0.0] # Output (ground truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code snippet, we declare variables to hold the state of our three neurons. A real implementation would typically be parameterized to be able to choose number of inputs, layers, and number of neurons in each layer, but all of those parameters are hardcoded in this example to focus on readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_w(input_count):\n",
    "    weights = np.zeros(input_count+1)\n",
    "    for i in range(1, (input_count+1)):\n",
    "        weights[i] = np.random.uniform(-1.0, 1.0)\n",
    "    return weights\n",
    "\n",
    "n_w = [neuron_w(2), neuron_w(2), neuron_w(2)]\n",
    "n_y = [0, 0, 0]\n",
    "n_error = [0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet starts with a function to print all the nine weights of the network (each print statement prints a three-element weight vector). The forward_pass function first computes the outputs of neurons 0 and 1 with the same inputs (the inputs from the training example) and then puts their outputs into an array, together with a bias value of 1.0, to use as input to neuron 2. That is, this function defines the topology of the network. We use tanh for the neurons in the first layer and the logistic sigmoid function for the output neuron.\n",
    "\n",
    "The backward_pass function starts by computing the derivative of the error function and then computes the derivative of the activation function for the output neuron. The error term of the output neuron is computed by multiplying these two together. We then continue to backpropagate the error to each of the two neurons in the hidden layer. This is done by computing the derivatives of their activation functions and multiplying these derivatives by the error term from the output neuron and by the weight to the output neuron.\n",
    "\n",
    "Finally, the adjust_weights function adjusts the weights for each of the three neurons. The adjustment factor is computed by multiplying the input by the learning rate and the error term for the neuron in question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_learning():\n",
    "    print('Current weights:')\n",
    "    for i, w in enumerate(n_w):\n",
    "        print('neuron ', i, ': w0 =', '%5.2f' % w[0],\n",
    "              ', w1 =', '%5.2f' % w[1], ', w2 =',\n",
    "              '%5.2f' % w[2])\n",
    "    print('----------------')\n",
    "\n",
    "def forward_pass(x):\n",
    "    global n_y\n",
    "    n_y[0] = np.tanh(np.dot(n_w[0], x)) # Neuron 0\n",
    "    n_y[1] = np.tanh(np.dot(n_w[1], x)) # Neuron 1\n",
    "    n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # 1.0 is bias\n",
    "    z2 = np.dot(n_w[2], n2_inputs)\n",
    "    n_y[2] = 1.0 / (1.0 + np.exp(-z2))\n",
    "\n",
    "def backward_pass(y_truth):\n",
    "    global n_error\n",
    "    error_prime = -(y_truth - n_y[2]) # Derivative of loss-func\n",
    "    derivative = n_y[2] * (1.0 - n_y[2]) # Logistic derivative\n",
    "    n_error[2] = error_prime * derivative\n",
    "    derivative = 1.0 - n_y[0]**2 # tanh derivative\n",
    "    n_error[0] = n_w[2][1] * n_error[2] * derivative\n",
    "    derivative = 1.0 - n_y[1]**2 # tanh derivative\n",
    "    n_error[1] = n_w[2][2] * n_error[2] * derivative\n",
    "\n",
    "def adjust_weights(x):\n",
    "    global n_w\n",
    "    n_w[0] -= (x * LEARNING_RATE * n_error[0])\n",
    "    n_w[1] -= (x * LEARNING_RATE * n_error[1])\n",
    "    n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # 1.0 is bias\n",
    "    n_w[2] -= (n2_inputs * LEARNING_RATE * n_error[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these pieces in place, the only remaining piece is the training loop shown in the code snippet below, which is somewhat similar to the training loop for the perceptron example in v2_2_perceptron_learning.\n",
    "\n",
    "We pick training examples in random order, call the functions forward_pass, backward_pass, and adjust_weights, and then print out the weights with the function show_learning. We adjust the weights regardless whether the network predicts correctly or not. Once we have looped through all four training examples, we check whether the network can predict them all correctly, and if not, we do another pass over them in random order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network training loop.\n",
    "all_correct = False\n",
    "while not all_correct: # Train until converged\n",
    "    all_correct = True\n",
    "    np.random.shuffle(index_list) # Randomize order\n",
    "    for i in index_list: # Train on all examples\n",
    "        forward_pass(x_train[i])\n",
    "        backward_pass(y_train[i])\n",
    "        adjust_weights(x_train[i])\n",
    "        show_learning() # Show updated weights\n",
    "    for i in range(len(x_train)): # Check if converged\n",
    "        forward_pass(x_train[i])\n",
    "        print('x1 =', '%4.1f' % x_train[i][1], ', x2 =',\n",
    "              '%4.1f' % x_train[i][2], ', y =',\n",
    "              '%.4f' % n_y[2])\n",
    "        if(((y_train[i] < 0.5) and (n_y[2] >= 0.5))\n",
    "                or ((y_train[i] >= 0.5) and (n_y[2] < 0.5))):\n",
    "            all_correct = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
